{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dtw-RVD0ekQs"},"outputs":[],"source":["# Line to save a loooooot of time\n","!pip install torch==1.10.0+cu111 torchvision==0.11.0+cu111 -f https://download.pytorch.org/whl/cu111/torch_stable.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hW3eYbZa0JH"},"outputs":[],"source":["# !rm mmdetection -r\n","!pip install -U sahi\n","!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html #check the current version of torch and cuda\n","                                                                                                #!!! only work with torch 1.x.0\n","!pip install mmdet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eIZdo9KiJucw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662551787420,"user_tz":-120,"elapsed":5907,"user":{"displayName":"Adrien Delhez","userId":"11782226645884779261"}},"outputId":"09387c3b-c0ef-411b-9568-3d7a38d2c63f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: opencv-python-headless<4.3 in /usr/local/lib/python3.7/dist-packages (4.2.0.34)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.6)\n"]}],"source":["import torch\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","import torchvision.transforms as T\n","import numpy as np\n","from PIL import Image\n","import cv2\n","\n","!pip install \"opencv-python-headless<4.3\"\n","from sahi.model import MmdetDetectionModel\n","from sahi.predict import get_sliced_prediction\n","from sahi.utils.cv import read_image_as_pil\n","from sahi.prediction import ObjectPrediction\n","\n","from sahi.utils.torch import is_torch_cuda_available, empty_cuda_cache"]},{"cell_type":"markdown","source":["# SAHI"],"metadata":{"id":"MAnFD_wZO8hv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_lvJ2ccIUvln"},"outputs":[],"source":["# Run these cells to use them below\n","# Class from the official code on the SAHI github\n","class DetectionModel:\n","    def __init__(\n","        self,\n","        model_path=None,\n","        config_path=None,\n","        device=None,\n","        mask_threshold: float = 0.5,\n","        prediction_score_threshold: float = 0.3,\n","        category_mapping=None,\n","        category_remapping=None,\n","        load_at_init: bool = True,\n","    ):\n","        \"\"\"\n","        Init object detection/instance segmentation model.\n","        Args:\n","            model_path: str\n","                Path for the instance segmentation model weight\n","            config_path: str\n","                Path for the mmdetection instance segmentation model config file\n","            device: str\n","                Torch device, \"cpu\" or \"cuda\"\n","            mask_threshold: float\n","                Value to threshold mask pixels, should be between 0 and 1\n","            prediction_score_threshold: float\n","                All predictions with score < prediction_score_threshold will be discarded\n","            category_mapping: dict: str to str\n","                Mapping from category id (str) to category name (str) e.g. {\"1\": \"pedestrian\"}\n","            category_remapping: dict: str to int\n","                Remap category ids based on category names, after performing inference e.g. {\"car\": 3}\n","            load_at_init: bool\n","                If True, automatically loads the model at initalization\n","        \"\"\"\n","        self.model_path = model_path\n","        self.config_path = config_path\n","        self.model = None\n","        self.device = device\n","        self.mask_threshold = mask_threshold\n","        self.prediction_score_threshold = prediction_score_threshold\n","        self.category_mapping = category_mapping\n","        self.category_remapping = category_remapping\n","        self._original_predictions = None\n","        self._object_prediction_list = None\n","\n","        # automatically set device if its None\n","        if not (self.device):\n","            self.device = \"cuda:0\" if is_torch_cuda_available() else \"cpu\"\n","\n","        # automatically load model if load_at_init is True\n","        if load_at_init:\n","            self.load_model()\n","\n","    def load_model(self):\n","        \"\"\"\n","        This function should be implemented in a way that detection model\n","        should be initialized and set to self.model.\n","        (self.model_path, self.config_path, and self.device should be utilized)\n","        \"\"\"\n","        NotImplementedError()\n","\n","    def unload_model(self):\n","        \"\"\"\n","        Unloads the model from CPU/GPU.\n","        \"\"\"\n","        self.model = None\n","        empty_cuda_cache()\n","\n","    def perform_inference(self, image: np.ndarray):\n","        \"\"\"\n","        This function should be implemented in a way that prediction should be\n","        performed using self.model and the prediction result should be set to self._original_predictions.\n","        Args:\n","            image: np.ndarray\n","                A numpy array that contains the image to be predicted.\n","        \"\"\"\n","        NotImplementedError()\n","\n","    def _create_object_prediction_list_from_original_predictions(\n","        self,\n","        shift_amount=[0, 0],\n","        full_shape=None,\n","    ):\n","        \"\"\"\n","        This function should be implemented in a way that self._original_predictions should\n","        be converted to a list of prediction.ObjectPrediction and set to\n","        self._object_prediction_list. self.mask_threshold can also be utilized.\n","        Args:\n","            shift_amount: list\n","                To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]\n","            full_image_size: list\n","                Size of the full image after shifting, should be in the form of [height, width]\n","        \"\"\"\n","        NotImplementedError()\n","\n","    def _apply_category_remapping(self):\n","        \"\"\"\n","        Applies category remapping based on mapping given in self.category_remapping\n","        \"\"\"\n","        # confirm self.category_remapping is not None\n","        assert (\n","            self.category_remapping is not None\n","        ), \"self.category_remapping cannot be None\"\n","        # remap categories\n","        for object_prediction in self._object_prediction_list:\n","            old_category_id_str = str(object_prediction.category.id)\n","            new_category_id_int = self.category_remapping[old_category_id_str]\n","            object_prediction.category.id = new_category_id_int\n","\n","    def convert_original_predictions(\n","        self,\n","        shift_amount=[0, 0],\n","        full_shape=None,\n","    ):\n","        \"\"\"\n","        Converts original predictions of the detection model to a list of\n","        prediction.ObjectPrediction object. Should be called after perform_inference().\n","        Args:\n","            shift_amount: list\n","                To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]\n","            full_shape: list\n","                Size of the full image after shifting, should be in the form of [height, width]\n","        \"\"\"\n","        self._create_object_prediction_list_from_original_predictions(\n","            shift_amount=shift_amount,\n","            full_shape=full_shape,\n","        )\n","        if self.category_remapping:\n","            self._apply_category_remapping()\n","\n","    @property\n","    def object_prediction_list(self):\n","        return self._object_prediction_list\n","\n","    @property\n","    def original_predictions(self):\n","        return self._original_predictions\n","\n","    def _create_predictions_from_object_prediction_list(object_prediction_list):\n","        \"\"\"\n","        This function should be implemented in a way that it converts a list of\n","        prediction.ObjectPrediction instance to detection model's original prediction format.\n","        Then returns the converted predictions.\n","        Can be considered as inverse of _create_object_prediction_list_from_predictions().\n","        Args:\n","            object_prediction_list: a list of prediction.ObjectPrediction\n","        Returns:\n","            original_predictions: a list of converted predictions in models original output format\n","        \"\"\"\n","        NotImplementedError()\n","\n","class MmdetDetectionModel(DetectionModel):\n","    def load_model(self):\n","        \"\"\"\n","        Detection model is initialized and set to self.model.\n","        \"\"\"\n","        from mmdet.apis import init_detector\n","\n","        # set model\n","        # model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=True) #ResNet50\n","        model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) #MobileNetv3\n","        # model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True) #MobileNetv3 320\n","        num_classes = 2  # 1 class (bird) + background\n","\n","        # get number of input features for the classifier\n","        in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","        # replace the pre-trained head with a new one\n","        currentdevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","        model.load_state_dict(torch.load(self.model_path, map_location=torch.device(currentdevice)))\n","        model.eval()\n","        self.model = model\n","\n","        # set category_mapping\n","        if not self.category_mapping:\n","            category_mapping = {\n","                str(ind): category_name\n","                for ind, category_name in enumerate(self.category_names)\n","            }\n","            self.category_mapping = category_mapping\n","\n","    def perform_inference(self, image: np.ndarray): #, image_size\n","        \"\"\"\n","        Prediction is performed using self.model and the prediction result is set to self._original_predictions.\n","        Args:\n","            image: np.ndarray\n","                A numpy array that contains the image to be predicted.\n","        \"\"\"\n","        # Confirm model is loaded\n","        assert (\n","            self.model is not None\n","        ), \"Model is not loaded, load it by calling .load_model()\"\n","\n","        # Supports only batch of 1\n","        # from mmdet.apis import inference_detector\n","\n","        # prediction_result = inference_detector(self.model, image)\n","        # print(\"Inference\")\n","        # self.model.eval()\n","        toTensor = T.Compose([T.ToTensor()])\n","        tensorimage = toTensor(image)\n","        tensorimage = tensorimage[None, ...]\n","        prediction_result = self.model(tensorimage)\n","\n","        self._original_predictions = prediction_result\n","\n","    @property\n","    def num_categories(self):\n","        \"\"\"\n","        Returns number of categories\n","        \"\"\"\n","        # if isinstance(self.model.CLASSES, str):\n","        #     num_categories = 1\n","        # else:\n","        #     num_categories = len(self.model.CLASSES)\n","        num_classes = 2\n","        return num_classes\n","\n","    @property\n","    def has_mask(self):\n","        \"\"\"\n","        Returns if model output contains segmentation mask\n","        \"\"\"\n","        has_mask = self.model.with_mask\n","        return has_mask\n","\n","    @property\n","    def category_names(self):\n","        # if self.num_categories == 1:\n","        #     return [self.model.CLASSES]\n","        # else:\n","        #     return self.model.CLASSES\n","        return [\"NoBird\", \"Bird\"]\n","\n","    def _create_object_prediction_list_from_original_predictions(\n","        self,\n","        shift_amount=[0, 0],\n","        full_shape=None,\n","    ):\n","        \"\"\"\n","        self._original_predictions is converted to a list of prediction.ObjectPrediction and set to\n","        self._object_prediction_list.\n","        Args:\n","            shift_amount: list\n","                To shift the box and mask predictions from sliced image to full sized image, should be in the form of [shift_x, shift_y]\n","            full_image_size: list\n","                Size of the full image after shifting, should be in the form of [height, width]\n","        \"\"\"\n","        fasterrcnn_predictions = self._original_predictions\n","\n","        object_prediction_list = []\n","\n","        # process predictions\n","        # print(\"Enter\")\n","        # print(fasterrcnn_predictions[0])\n","        keepidx = torchvision.ops.nms(fasterrcnn_predictions[0]['boxes'], fasterrcnn_predictions[0]['scores'], 0.0).cpu().detach().numpy().astype(np.int32)\n","\n","        boxes = fasterrcnn_predictions[0][\"boxes\"].cpu().detach().numpy().astype(np.int32)\n","        labels =  fasterrcnn_predictions[0][\"labels\"].cpu().detach().numpy().astype(np.int32)\n","        scores =  fasterrcnn_predictions[0][\"scores\"].cpu().detach().numpy()\n","        boxes = boxes[keepidx]\n","        scores = scores[keepidx]\n","        labels = labels[keepidx]\n","\n","        confidenceidx = [idx for idx, elt in enumerate(scores) if elt >= 0.5]\n","        boxes = boxes[confidenceidx]\n","        scores = scores[confidenceidx]\n","        labels = labels[confidenceidx]\n","\n","        for idx, bbox in enumerate(boxes):\n","          object_prediction = ObjectPrediction(\n","              bbox=bbox,\n","              category_id=int(labels[idx]),\n","              score=scores[idx],\n","              category_name=self.category_names[int(labels[idx])],\n","              shift_amount=shift_amount,\n","              full_shape=full_shape,\n","          )\n","\n","          # append ObjectPrediction object to object_prediction_list\n","          object_prediction_list.append(object_prediction)\n","\n","        self._object_prediction_list = object_prediction_list\n","\n","    def _create_original_predictions_from_object_prediction_list(\n","        self,\n","        object_prediction_list,\n","    ):\n","        \"\"\"\n","        Converts a list of prediction.ObjectPrediction instance to detection model's original prediction format.\n","        Then returns the converted predictions.\n","        Can be considered as inverse of _create_object_prediction_list_from_predictions().\n","        Args:\n","            object_prediction_list: a list of prediction.ObjectPrediction\n","        Returns:\n","            original_predictions: a list of converted predictions in models original output format\n","        \"\"\"\n","        # init variables\n","        boxes = []\n","        masks = []\n","        num_categories = self.num_categories\n","        category_id_list = np.arange(num_categories)\n","        category_id_to_bbox = {category_id: [] for category_id in category_id_list}\n","        category_id_to_mask = {category_id: [] for category_id in category_id_list}\n","        # form category_to_bbox and category_to_mask dicts from object_prediction_list\n","        for object_prediction in object_prediction_list:\n","            category_id = object_prediction.category.id\n","            # form bbox as 1x5 list [xmin, ymin, xmax, ymax, score]\n","            bbox = object_prediction.bbox.to_voc_bbox()\n","            bbox.extend([object_prediction.score.score])\n","            category_id_to_bbox[category_id].append(np.array(bbox, dtype=np.float32))\n","            # form 2d bool mask\n","            if self.has_mask:\n","                mask = object_prediction.mask.bool_mask\n","                category_id_to_mask[category_id].append(mask)\n","\n","        for category_id in category_id_to_bbox.keys():\n","            if not category_id_to_bbox[category_id]:\n","                # add 0x5 array to boxes for empty categories\n","                boxes.append(np.zeros((0, 5), dtype=np.float32))\n","                if self.has_mask:\n","                    masks.append([])\n","            else:\n","                # form boxes and masks\n","                boxes.append(np.array(category_id_to_bbox[category_id]))\n","                if self.has_mask:\n","                    masks.append(np.array(category_id_to_mask[category_id]))\n","        # form final output\n","        if self.has_mask:\n","            original_predictions = (boxes, masks)\n","        else:\n","            original_predictions = boxes\n","\n","        return original_predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zt3YGiX3BubB"},"outputs":[],"source":["def SAHIalgorithm(img, detection_model):\n","  \"\"\" Apply the SAHI algorithm on the image and return the boxes.\n","  \"\"\"\n","  result = get_sliced_prediction(\n","    img,\n","    detection_model,\n","    slice_height = 224,\n","    slice_width = 224,\n","    overlap_height_ratio = 0.1,\n","    overlap_width_ratio = 0.1\n","  )\n","  cocodict = result.to_coco_predictions()\n","\n","  # Transform the boxes in PascalVOC format\n","  boxes = []\n","  for i in cocodict:\n","    box = i[\"bbox\"]\n","    boxes.append([box[0], box[1], box[0] + box[2], box[1] + box[3]])\n","\n","  return boxes"]},{"cell_type":"markdown","source":["# Video Detection With SAHI (version that save the predictions not the video)"],"metadata":{"id":"ydS_jGRyOoYr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5fvqfh93XtT"},"outputs":[],"source":["from google.colab.patches import cv2_imshow\n","##############################################################################\n","# Video Detection With SAHI (version that save the predictions not the video) #\n","##############################################################################\n","devicec = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(devicec)\n","# ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/\"\n","# MODEL_NAME = \"bestfasterrcnnv2_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch50_8020_rc.pth\"\n","# MODEL_NAME = \"bestMobileNetv3_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100_8020_rc.pth\"\n","# MODEL_NAME = \"bestMobileNetv3320_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100_8020_rc.pth\"\n","\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/v1/\"\n","# MODEL_NAME = \"bestfasterrcnnv1_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","MODEL_NAME = \"bestfasterrcnnv1_MobileNetv3_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","checkpoint = ROOT_DIR_SAVING + MODEL_NAME\n","model = MmdetDetectionModel(\n","    # model_path=mmdet_cascade_mask_rcnn_model_path,\n","    model_path=checkpoint,\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",")\n","\n","print(\"------------Load------------\")\n","vid = cv2.VideoCapture(\"/content/drive/MyDrive/Thesis/videos/presentationvideos/starlings4.mov\")\n","# vid = cv2.VideoCapture(\"/content/drive/MyDrive/Thesis/videos/A4.mp4\")\n","\n","count=0\n","iter = 0\n","all_frames = []\n","all_boxes = []\n","all_birdamt = []\n","while vid.isOpened():\n","    # print(\"Enter\")\n","    ret, orig_frame = vid.read()\n","    # print(ret)\n","    if ret == True:\n","        orig_shape = orig_frame.shape\n","        # orig_frame = orig_frame[200:200+576, 1700:1700+576]\n","        print(orig_shape)\n","        # frame = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2RGB).astype(np.float32) # array rgb\n","        pil_frame = Image.fromarray(orig_frame, \"RGB\")\n","        # pil_frame = pil_frame.crop((200, 1700, 200+576, 1700+576))\n","        pil_frame = read_image_as_pil(pil_frame)\n","\n","        boxes = SAHIalgorithm(pil_frame, model)\n","\n","        # loop over the boxes\n","        for idx, box in enumerate(boxes):\n","            cv2.rectangle(orig_frame,\n","                        (int(box[0]), int(box[1])),\n","                        (int(box[2]), int(box[3])),\n","                        (255, 0, 0), 1)\n","            # cv2.putText(orig_frame, \"Bird Amount:\"+ str(len(boxes)), (100, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 3)\n","            cv2.putText(orig_frame, str(len(boxes)), (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 3)\n","            # cv2.putText(orig_frame, str(int((int(box[0])+int(box[1]))/2))+\"x\"+str(int((int(box[2])+int(box[3]))/2)), (int(box[0])-10, int(box[3])+10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,0,0), 1)\n","            # cv2.putText(orig_frame, str(idx+1), (int(box[0]), int(box[1])-5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,0,0), 1)\n","        # fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","        # ax.imshow(cv2.cvtColor(orig_frame, cv2.COLOR_RGB2BGR))\n","        # # plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/\"+str(image_id)+\"area\" + \".png\")\n","        # ax.axis('off')\n","        # plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/imgv6ResNetSahi.jpg\", bbox_inches='tight')\n","        # plt.show()\n","\n","\n","        size = (orig_shape[0], orig_shape[1])\n","        all_frames.append(orig_frame)\n","        all_boxes.append(boxes)\n","        iter += 1\n","        imS = cv2.resize(orig_frame, (960, 540))\n","        cv2_imshow(imS)\n","\n","        count+=1\n","        if ((count % 5) == 0): # Saving for security\n","          print(\"Saved\")\n","          PSAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/presentationvideos/starlings4.npy\"\n","          data = np.asarray([all_birdamt, all_boxes])\n","          import pickle as cPickle\n","          with open(PSAVE_PATH, 'wb') as outp:  # Overwrites any existing file.\n","              cPickle.dump(data, outp, cPickle.HIGHEST_PROTOCOL)\n","        if 0xFF == ord('q'):\n","          print(\"Break\")\n","          break\n","    else:\n","        break\n","\n","print(\"----------Prediction Done----------\")\n","print(\"Saving\")\n","PSAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/presentationvideos/predstarlings4.npy\"\n","data = np.asarray([all_birdamt, all_boxes])\n","import pickle as cPickle\n","with open(PSAVE_PATH, 'wb') as outp:  # Overwrites any existing file.\n","    cPickle.dump(data, outp, cPickle.HIGHEST_PROTOCOL)\n","print(\"--------------Saved----------------\")"]},{"cell_type":"markdown","source":["Add the information on video based on a file"],"metadata":{"id":"dRvSsok7Os_d"}},{"cell_type":"code","source":["##################################\n","# Add saved information on video #\n","##################################\n","!pip3 install pickle5\n","import pickle5 as cPickle\n","# import pickle as cPickle\n","import cv2\n","\n","PSAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/presentationvideos/finalresult/starlings.npy\"\n","file = open(PSAVE_PATH,'rb')\n","dataPickle = file.read()\n","file.close()\n","res = cPickle.loads(dataPickle)\n","\n","all_boxes = res[1]\n","\n","vid = cv2.VideoCapture(\"/content/drive/MyDrive/Thesis/videos/presentationvideos/starlings.mov\")\n","# vid = cv2.VideoCapture(\"/content/drive/MyDrive/Thesis/videos/A4.mp4\")\n","\n","print(\"------Putting infos on frames------\")\n","frameidx = 0\n","all_frames = []\n","while vid.isOpened():\n","    # print(\"Enter\")\n","    ret, orig_frame = vid.read()\n","    # print(ret)\n","    if ret == True:\n","        orig_shape = orig_frame.shape\n","\n","        if len(all_boxes) == frameidx:\n","            break\n","        \n","        boxes = all_boxes[frameidx]\n","        \n","        # loop over the boxes\n","        for idx, box in enumerate(boxes):\n","            cv2.rectangle(orig_frame,\n","                        (int(box[0]), int(box[1])),\n","                        (int(box[2]), int(box[3])),\n","                        (255, 0, 0), 1)\n","            # cv2.putText(orig_frame, \"Bird Amount:\"+ str(len(boxes)), (100, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 3)\n","            cv2.putText(orig_frame, str(len(boxes)), (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 3)\n","            cv2.putText(orig_frame, str(int((int(box[0])+int(box[1]))/2))+\"x\"+str(int((int(box[2])+int(box[3]))/2)), (int(box[0])-10, int(box[3])+10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,0,0), 1)\n","            # cv2.putText(orig_frame, str(idx+1), (int(box[0]), int(box[1])-5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,0,0), 1)\n","\n","        size = (orig_shape[0], orig_shape[1])\n","        all_frames.append(orig_frame)\n","        frameidx += 1\n","\n","        if 0xFF == ord('q'):\n","            break\n","    else:\n","        break\n","\n","print(\"------Saving the video------\")\n","SAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/presentationvideos/predstarlings.mp4\"\n","size = (size[1], size[0])\n","fps = 30\n","out = cv2.VideoWriter(SAVE_PATH,cv2.VideoWriter_fourcc(*'mp4v'), fps, size)\n","print(\"Video \"+str(fps)+\" fps.\")\n","\n","for i in range(len(all_frames)):\n","    out.write(all_frames[i])\n","out.release()\n","print(\"------Finish------\")"],"metadata":{"id":"CL5rDKYc3DSv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662551913714,"user_tz":-120,"elapsed":59116,"user":{"displayName":"Adrien Delhez","userId":"11782226645884779261"}},"outputId":"aa3b23b7-09f1-4de3-c6a8-458012c80667"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pickle5\n","  Using cached pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n","Installing collected packages: pickle5\n","Successfully installed pickle5-0.0.12\n","------Putting infos on frames------\n","------Saving the video------\n","Video 30 fps.\n","------Finish------\n"]}]},{"cell_type":"markdown","source":["# Video Detection with SAHI"],"metadata":{"id":"zlqX33yEMD3i"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOU3Jyo--4yg"},"outputs":[],"source":["from google.colab.patches import cv2_imshow\n","############################\n","# Video Detection With SAHI#\n","############################\n","def saveimages(all_frames, size):\n","  SAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/prediction/predictionACs2_Batch8_Epoch50_224.mp4\"\n","  size = (size[1], size[0])\n","  out = cv2.VideoWriter(SAVE_PATH,cv2.VideoWriter_fourcc(*'mp4v'), 20, size)\n","  \n","  for i in range(len(all_frames)):\n","      out.write(all_frames[i])\n","  out.release()\n","\n","devicec = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(devicec)\n","# ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/\"\n","# MODEL_NAME = \"bestfasterrcnnv2_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch50_8020_rc.pth\"\n","# MODEL_NAME = \"bestMobileNetv3_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100_8020_rc.pth\"\n","# MODEL_NAME = \"bestMobileNetv3320_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100_8020_rc.pth\"\n","\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/v1/\"\n","MODEL_NAME = \"bestfasterrcnnv1_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","# MODEL_NAME = \"bestfasterrcnnv1_MobileNetv3_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","checkpoint = ROOT_DIR_SAVING + MODEL_NAME\n","model = MmdetDetectionModel(\n","    # model_path=mmdet_cascade_mask_rcnn_model_path,\n","    model_path=checkpoint,\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",")\n","\n","print(\"------------Load------------\")\n","vid = cv2.VideoCapture(\"/content/drive/MyDrive/Thesis/videos/A10.mp4\")\n","\n","count=0\n","iter = 0\n","all_frames = []\n","all_boxes = []\n","while vid.isOpened():\n","    # print(\"Enter\")\n","    ret, orig_frame = vid.read()\n","    # print(ret)\n","    if ret == True:\n","        orig_shape = orig_frame.shape\n","        # orig_frame = orig_frame[200:200+576, 1700:1700+576]\n","        print(orig_shape)\n","        # frame = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2RGB).astype(np.float32) # array rgb\n","        pil_frame = Image.fromarray(orig_frame, \"RGB\")\n","        # pil_frame = pil_frame.crop((200, 1700, 200+576, 1700+576))\n","        pil_frame = read_image_as_pil(pil_frame)\n","\n","        boxes = SAHIalgorithm(pil_frame, model)\n","\n","        # loop over the boxes\n","        for idx, box in enumerate(boxes):\n","            cv2.rectangle(orig_frame,\n","                        (int(box[0]), int(box[1])),\n","                        (int(box[2]), int(box[3])),\n","                        (255, 0, 0), 1)\n","            # cv2.putText(orig_frame, \"Bird Amount:\"+ str(len(boxes)), (100, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 3)\n","            cv2.putText(orig_frame, str(len(boxes)), (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 3)\n","            # cv2.putText(orig_frame, str(int((int(box[0])+int(box[1]))/2))+\"x\"+str(int((int(box[2])+int(box[3]))/2)), (int(box[0])-10, int(box[3])+10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,0,0), 1)\n","            # cv2.putText(orig_frame, str(idx+1), (int(box[0]), int(box[1])-5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,0,0), 1)\n","        # fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","        # ax.imshow(cv2.cvtColor(orig_frame, cv2.COLOR_RGB2BGR))\n","        # # plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/\"+str(image_id)+\"area\" + \".png\")\n","        # ax.axis('off')\n","        # plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/imgv6ResNetSahi.jpg\", bbox_inches='tight')\n","        # plt.show()\n","\n","\n","        size = (orig_shape[0], orig_shape[1])\n","        all_frames.append(orig_frame)\n","        all_boxes.append(boxes)\n","        iter += 1\n","        imS = cv2.resize(orig_frame, (960, 540))\n","        # imS = cv2.resize(orig_frame, (576, 576))\n","        cv2_imshow(imS)\n","        # cv2.waitKey(1)\n","        # # Press Q on keyboard to  exit\n","        # if cv2.waitKey(500) & 0xFF == ord('q'):\n","        #     break\n","        saveimages(all_frames, size)\n","        count+=1\n","        if ((count % 5) == 0): # Save security\n","          print(\"Saved\")\n","          SAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/prediction/predvid6Mobile224.mp4\"\n","          size = (size[1], size[0])\n","          out = cv2.VideoWriter(SAVE_PATH,cv2.VideoWriter_fourcc(*'mp4v'), 10, size)\n","          \n","          for i in range(len(all_frames)):\n","              out.write(all_frames[i])\n","          out.release()\n","        if 0xFF == ord('q'):\n","            break\n","    else:\n","        break\n","\n","print(\"----------Prediction Done----------\")\n","saveimages(all_frames, size)\n","print(\"--------------Saved----------------\")"]},{"cell_type":"markdown","source":["# Image Detection"],"metadata":{"id":"x130WQe6L59s"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kgmnR3dczEW"},"outputs":[],"source":["###################\n","# Image Detection #\n","###################\n","import time\n","import matplotlib.pyplot as plt\n","\n","devicec = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(devicec)\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/\"\n","# MODEL_NAME = \"bestfasterrcnnv2_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch50_8020_rc.pth\"\n","# MODEL_NAME = \"bestMobileNetv3_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100_8020_rc.pth\"\n","# MODEL_NAME = \"bestMobileNetv3320_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100_8020_rc.pth\"\n","\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/v1/\"\n","# MODEL_NAME = \"bestfasterrcnnv1_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","MODEL_NAME = \"bestfasterrcnnv1_MobileNetv3_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","# MODEL_NAME = \"bestfasterrcnnv1_MobileNetv3320_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","checkpoint = ROOT_DIR_SAVING + MODEL_NAME\n","model = MmdetDetectionModel(\n","    # model_path=mmdet_cascade_mask_rcnn_model_path,\n","    model_path=checkpoint,\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",")\n","\n","orig_frame = cv2.imread('/content/drive/MyDrive/Thesis/videos/video2image/v5/v5img12.jpg', 1)\n","orig_shape = orig_frame.shape\n","print(orig_shape)\n","# frame = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2RGB).astype(np.float32) # array rgb\n","pil_frame = Image.fromarray(orig_frame, \"RGB\")\n","pil_frame = read_image_as_pil(pil_frame)\n","\n","start = time.time()\n","boxes = SAHIalgorithm(pil_frame, model)\n","end = time.time()\n","print(\"Time for SAHI on one image\", end - start)\n","\n","fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","sample = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2RGB)\n","print(\"Size\", sample.shape)\n","for box in boxes:\n","    cv2.rectangle(sample,\n","                (int(box[0]), int(box[1])),\n","                (int(box[2]), int(box[3])),\n","                (0, 0, 255), 2)\n","    \n","ax.imshow((sample).astype(np.uint8))\n","# plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/\"+str(image_id)+\"area\" + \".png\")\n","ax.axis('off')\n","plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/imagerapport/imgv5ResNetSAHI.pdf\", bbox_inches='tight')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zX589fbLn6nH"},"outputs":[],"source":["SAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/prediction/predvid7ResNet.mp4\"\n","size = (size[1], size[0])\n","out = cv2.VideoWriter(SAVE_PATH,cv2.VideoWriter_fourcc(*'mp4v'), 20, size)\n"," \n","for i in range(len(all_frames)):\n","    out.write(all_frames[i])\n","out.release()"]},{"cell_type":"markdown","metadata":{"id":"a5qzzBUQdNk9"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"mount_file_id":"1keFLpnHwwAbWNcprQLO7PVO2UuBmlPH_","authorship_tag":"ABX9TyOBVR7/U2Jmlp2MQKdKfe6R"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}