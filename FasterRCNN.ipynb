{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FasterRCNN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1i9J2iFNbyJj-Z2c4DjQE6Nh6jchrSGsA","authorship_tag":"ABX9TyMuhmXOwV0vQ+qmoGB4rMFA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdRwPIDA0nWB","executionInfo":{"status":"ok","timestamp":1645724510431,"user_tz":-60,"elapsed":18698,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"ed084c7a-b121-4bda-b28b-8d3674b0d0a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","import xml.etree.ElementTree as ET\n","import pickle as cPickle\n","import argparse\n","\n","import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.rpn import AnchorGenerator\n","import torchvision.transforms as T\n","\n","import torch.nn as nn\n","import torch\n","\n","!pip install albumentations==0.4.6\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import sys\n","sys.path.insert(0,\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/dataset\")"],"metadata":{"id":"4KJo128n1INW","executionInfo":{"status":"ok","timestamp":1645893594604,"user_tz":-60,"elapsed":4820,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"107d952a-88e4-41c9-8b49-a1dce363a69e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n","Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.5)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.2.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.7)\n"]}]},{"cell_type":"code","source":["# Manage multiple versions of python with pip\n","# py -3.8 -m pip install package\n","#https://stackoverflow.com/questions/2812520/dealing-with-multiple-python-versions-and-pip\n","# Inspired by torchvision example: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n","\n","class BirdDataset(torch.utils.data.Dataset):\n","    \"\"\"Class to charecterize the bird dataset\"\"\"\n","\n","    def __init__(self, root_dir, transforms=None):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.root = root_dir\n","        self.transforms = transforms\n","        \n","        self.imgs = list(sorted(os.listdir(os.path.join(root_dir, \"all_images\")), key=lambda x: int(os.path.splitext(x)[0])))  # list of all image names - jpg\n","        self.boxes = list(sorted(os.listdir(os.path.join(root_dir, \"all_labels\")), key=lambda x: int(os.path.splitext(x)[0]))) # list of all image names - xml\n","    \n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Loads and returns a sample from the dataset at the given index idx\"\"\"\n","        # load images and boxes\n","        img_path = os.path.join(self.root, \"all_images\", self.imgs[idx])\n","        box_path = os.path.join(self.root, \"all_labels\", self.boxes[idx])\n","        # print(\"Image path\", img_path)\n","        # print(type(cv2.imread(img_path, cv2.IMREAD_COLOR)))\n","        # img = Image.open(img_path).convert(\"RGB\")\n","        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        img /= 255.0\n","        \n","        # get boxes for each bird\n","        document = ET.parse(box_path)\n","        root = document.getroot()\n","        boxes = []\n","        for item in root.findall(\".//object/bndbox\"):\n","            xmin = float(item.find('xmin').text)\n","            xmax = float(item.find('xmax').text)\n","            ymin = float(item.find('ymin').text)\n","            ymax = float(item.find('ymax').text)\n","\n","            box = [xmin, ymin, xmax, ymax]\n","            boxes.append(box)\n","        num_objs = len(boxes)\n","\n","        # convert everything into a torch.Tensor\n","        image_id = torch.tensor([idx+1])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.ones((num_objs,), dtype=torch.int64) # only one class : a bird\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        # target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        # target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            # img = self.transforms(img)\n","            sample = {\n","                'image': img,\n","                'bboxes': target['boxes'],\n","                'labels': labels\n","            }\n","            sample = self.transforms(**sample)\n","\n","            img = sample['image']\n","            if len(sample['bboxes']) == 0: # \n","                target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n","            else:\n","                target['boxes'] = torch.tensor(sample['bboxes'])\n","\n","        return img, target"],"metadata":{"id":"SIZlc4Ut1rTV","executionInfo":{"status":"ok","timestamp":1645894789521,"user_tz":-60,"elapsed":597,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def get_model(trained=True, save_path=None):\n","  device = 'cpu'\n","  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","  num_classes = 2  # 1 class (bird) + background\n","\n","  # get number of input features for the classifier\n","  in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","  # replace the pre-trained head with a new one\n","  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","  model.to(device)\n","\n","  if trained:\n","    if save_path == None: print(\"No path tto the saved model\")\n","    model.load_state_dict(torch.load(save_path))\n","\n","  return model"],"metadata":{"id":"do6oPKRF4jfj","executionInfo":{"status":"ok","timestamp":1645893614997,"user_tz":-60,"elapsed":7,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#########\n","# UTILS #\n","#########\n","def collate_fn(batch):\n","  \"\"\"Necessary for DataLoader\"\"\"\n","  return tuple(zip(*batch))\n","\n","def get_transform(train):\n","  \"\"\"Tansform the training and test set. Data Augmentation is made here.\"\"\"\n","  transforms = []\n","  transforms.append(A.Resize(224, 224, interpolation = cv2.INTER_LANCZOS4))\n","  if train:\n","      print(\"\")\n","      # transforms.append(A.RandomCrop(width=576, height=576))\n","      # transforms.append(A.Flip(0.5))\n","      # transforms.append(A.Normalize(mean=[0.598, 0.554, 0.508], std=[0.090, 0.081, 0.076]))           \n","  transforms.append(ToTensorV2(p=1.0)) \n","  return A.Compose(transforms, bbox_params={'format': 'pascal_voc', 'min_visibility': 0.6, 'label_fields': ['labels']})\n","\n","def save_performance(score, filename):\n","  \"\"\"Save the scores (4) on a txt file of name filename\"\"\"\n","  return 0\n","\n","class Performance():\n","    \"\"\"Class to calculate and store the performance/score of a model\"\"\"\n","    def __init__(self, root_save, args):\n","        \"\"\"\n","        Args:\n","            root_save (string): \n","            params (callable, optional):\n","        \"\"\"\n","        self.root_save = root_save\n","        self.args = args\n","        self.train_score = [[],[],[],[]]\n","        self.validation_score = [[],[],[],[]]\n","        self.training_iou = []\n","        self.test_iou = []\n","\n","    def add_score(self, values, training):\n","      \"\"\"\n","      Add the different score of the model for every iteration to the corresponding list of score\n","      values : dictionnary containing  'loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg'\n","      \"\"\"\n","      if training:\n","        self.train_score[0].append(values['loss_classifier'])\n","        self.train_score[1].append(values['loss_box_reg'])\n","        self.train_score[2].append(values['loss_objectness'])\n","        self.train_score[3].append(values['loss_rpn_box_reg'])\n","      else:    \n","        self.train_score[0].append(values['loss_classifier'])\n","        self.train_score[1].append(values['loss_box_reg'])\n","        self.train_score[2].append(values['loss_objectness'])\n","        self.train_score[3].append(values['loss_rpn_box_reg'])\n","    \n","    def add_accuracy(self, val, training):\n","      if training:\n","        self.training_iou.append(val)\n","      else:\n","        self.test_iou.append(val)\n","\n","    def save(self, obj):\n","        \"\"\"Save the class in txt file\"\"\"\n","        with open(self.root_save, 'wb') as outp:  # Overwrites any existing file.\n","            cPickle.dump(obj, outp, cPickle.HIGHEST_PROTOCOL)\n","\n","    # def load(self):\n","    #     \"\"\"Load the class from txt file\"\"\"\n","    #     file = open(self.root_save+'.txt','r')\n","    #     dataPickle = file.read()\n","    #     file.close()\n","\n","    #     self.__dict__ = cPickle.loads(dataPickle)"],"metadata":{"id":"2ft0imD_SqZy","executionInfo":{"status":"ok","timestamp":1645897361468,"user_tz":-60,"elapsed":244,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# Global variable\n","ROOT_DIR_DATA = \"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/dataset\"\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/savedmodel\"\n","SAVEDMODEL_NAME = \"/fasterrcnn_SGD0005_None_Batch8_Epoch2.pth\"\n","SAVEDPERFORMANCE_NAME = \"/fasterrcnn_SGD0005_Batch8_Epoch2.pth\"\n","\n","# instantiate dataset objects\n","ds = BirdDataset(ROOT_DIR_DATA, get_transform(train=True))\n","ds_test = BirdDataset(ROOT_DIR_DATA, get_transform(train=False))\n","\n","# set hyper-parameters\n","num_epochs = 1\n","num_classes = 2\n","num_coord = 4\n","num_workers = 2\n","batch_size = 8\n","\n","# instantiate data loaders\n","# split the dataset in train and test set\n","random_seed = 1 # or any of your favorite number \n","torch.manual_seed(random_seed)\n","indices = torch.randperm(len(ds)).tolist()\n","print(indices)\n","# torch.manual_seed(random_seed)\n","# indices = torch.randperm(len(ds)).tolist()\n","# print(indices)\n","# indices = torch.randperm(len(ds)).tolist()\n","# print(indices)\n","dataset = torch.utils.data.Subset(ds, indices[:-50][0:7])\n","dataset_test = torch.utils.data.Subset(ds_test, indices[-50:])\n","\n","# define training and validation data loaders\n","data_loader_training = torch.utils.data.DataLoader(dataset, shuffle=True, collate_fn=collate_fn, \n","                                                   num_workers=num_workers, batch_size=batch_size)\n","data_loader_test = torch.utils.data.DataLoader(dataset_test, shuffle=True, collate_fn=collate_fn,\n","                                               num_workers=num_workers, batch_size=batch_size)"],"metadata":{"id":"cm3-m2n_RgfT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645896909340,"user_tz":-60,"elapsed":317,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"5d40c610-3ba4-4eda-fef1-28852d4ae3c6"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[21, 130, 236, 536, 311, 18, 247, 371, 223, 513, 230, 95, 220, 159, 279, 498, 278, 419, 86, 307, 446, 554, 336, 597, 411, 253, 150, 618, 61, 33, 366, 152, 4, 557, 483, 266, 519, 355, 514, 464, 645, 475, 37, 605, 647, 416, 94, 302, 433, 377, 380, 51, 388, 212, 203, 463, 367, 238, 219, 305, 578, 275, 245, 308, 277, 167, 77, 168, 516, 649, 297, 109, 476, 177, 144, 83, 561, 395, 539, 369, 479, 113, 384, 293, 14, 496, 495, 494, 602, 176, 158, 169, 441, 590, 429, 434, 263, 5, 52, 78, 173, 469, 96, 328, 576, 505, 216, 55, 195, 357, 92, 206, 457, 188, 75, 407, 626, 531, 312, 89, 472, 567, 337, 372, 577, 226, 359, 8, 552, 389, 555, 122, 82, 244, 414, 537, 652, 224, 381, 126, 54, 228, 218, 526, 72, 7, 344, 386, 368, 123, 271, 365, 644, 334, 378, 620, 582, 426, 24, 199, 139, 296, 617, 530, 131, 56, 390, 298, 292, 601, 198, 157, 242, 232, 191, 125, 194, 329, 471, 129, 436, 600, 629, 453, 466, 452, 314, 63, 634, 137, 632, 499, 503, 40, 592, 585, 607, 541, 23, 508, 141, 375, 412, 331, 589, 290, 393, 65, 76, 490, 286, 569, 98, 84, 9, 625, 529, 259, 507, 650, 313, 512, 231, 415, 398, 80, 35, 103, 563, 520, 265, 533, 145, 428, 454, 59, 71, 30, 42, 146, 341, 473, 281, 135, 257, 599, 46, 506, 481, 543, 345, 447, 221, 423, 491, 409, 119, 562, 586, 500, 44, 538, 462, 239, 350, 353, 243, 547, 287, 431, 549, 193, 208, 90, 542, 363, 118, 591, 85, 34, 596, 250, 304, 593, 432, 110, 264, 642, 197, 504, 654, 217, 155, 25, 598, 394, 175, 468, 291, 26, 588, 621, 404, 60, 50, 280, 478, 383, 326, 522, 427, 400, 442, 480, 528, 348, 142, 160, 204, 574, 300, 140, 276, 373, 364, 655, 62, 421, 392, 170, 559, 166, 309, 651, 443, 174, 127, 284, 525, 187, 548, 66, 608, 635, 342, 374, 524, 31, 149, 518, 484, 628, 237, 301, 29, 205, 493, 391, 382, 136, 106, 310, 10, 319, 79, 636, 156, 470, 418, 13, 540, 430, 133, 445, 346, 20, 289, 315, 354, 87, 12, 352, 323, 185, 523, 184, 69, 114, 99, 227, 295, 325, 117, 43, 614, 249, 15, 406, 553, 609, 317, 256, 643, 235, 358, 339, 532, 134, 11, 489, 324, 16, 565, 535, 215, 57, 41, 558, 408, 107, 612, 437, 100, 420, 64, 349, 545, 321, 619, 581, 274, 162, 233, 640, 164, 338, 606, 637, 213, 630, 3, 444, 438, 47, 283, 28, 639, 550, 396, 546, 251, 458, 570, 179, 282, 459, 120, 53, 332, 343, 1, 171, 67, 440, 439, 73, 91, 527, 202, 38, 497, 449, 2, 335, 544, 502, 190, 333, 207, 181, 270, 58, 631, 572, 340, 93, 370, 477, 604, 485, 322, 351, 583, 248, 486, 320, 474, 115, 456, 580, 568, 623, 108, 252, 211, 255, 575, 36, 68, 273, 19, 200, 571, 413, 646, 492, 587, 566, 246, 622, 262, 269, 180, 451, 45, 143, 487, 362, 153, 402, 455, 138, 104, 511, 288, 482, 0, 116, 450, 627, 132, 214, 225, 39, 613, 379, 399, 210, 209, 254, 178, 361, 560, 467, 385, 165, 161, 97, 294, 101, 260, 258, 17, 356, 128, 189, 234, 573, 376, 172, 151, 410, 222, 564, 387, 124, 610, 425, 105, 509, 48, 111, 186, 515, 49, 595, 268, 241, 579, 360, 653, 534, 603, 201, 633, 272, 240, 448, 192, 6, 102, 306, 316, 517, 81, 460, 401, 261, 229, 32, 347, 148, 403, 615, 521, 405, 303, 594, 285, 641, 638, 422, 461, 584, 648, 435, 163, 417, 299, 318, 330, 624, 424, 182, 556, 183, 611, 510, 196, 70, 74, 154, 616, 27, 112, 327, 397, 147, 267, 88, 551, 501, 121, 22, 488, 465]\n"]}]},{"cell_type":"code","source":["#################\n","# Visualisation #\n","#################\n","device = \"cpu\"\n","images, targets = next(iter(data_loader_training))\n","# for images, targets in data_loader_training:\n","images = list(image.to(device) for image in images)\n","targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","# print(targets)\n","\n","for i in range(len(images)):\n","    # print(targets[i]['boxes'].size())\n","    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n","    image_id = targets[i]['image_id'].cpu().numpy().astype(np.int32)\n","    area = targets[i]['area'].cpu().numpy().astype(np.int32)\n","    print(image_id)\n","    # print(\"Area\", area)\n","    # print(images[i].size())\n","    sample = images[i].permute(1,2,0).cpu().numpy()\n","    # print(sample)\n","    # print(sample.shape)\n","    # print(\"Box\", boxes)\n","\n","    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","    sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n","    for box in boxes:\n","        cv2.rectangle(sample,\n","                    (int(box[0]), int(box[1])),\n","                    (int(box[2]), int(box[3])),\n","                    (1, 0, 0), 1)\n","        \n","    ax.imshow((sample * 255).astype(np.uint8))\n","    plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/\"+str(image_id)+\"area\" + \".png\")\n","    plt.show()"],"metadata":{"id":"CN4ImOk-22lV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#################################\n","# Training and Testing function #\n","#################################\n","def training(args):\n","  ds = BirdDataset(ROOT_DIR_DATA, get_transform(train=True))\n","  data_loader_training = torch.utils.data.DataLoader(dataset, shuffle=True, collate_fn=collate_fn, \n","                                                   num_workers=num_workers, batch_size=args.batch_size)\n","\n","  device = \"cpu\"\n","  model = get_model(trained=False)\n","\n","  params = [p for p in model.parameters() if p.requires_grad]\n","  optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","  # optimizer = torch.optim.Adam(params, lr=0.001)\n","  # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","  # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n","  lr_scheduler = None\n","\n","  itr = 1\n","  epoch_loss = 0\n","  perf = Performance(ROOT_DIR_SAVING + SAVEDPERFORMANCE_NAME, args)\n","\n","  for epoch in range(args.epochs):\n","      epoch_loss = 0\n","      iteration = 0\n","      model.train()\n","\n","      # Training\n","      for images, targets in data_loader_training:\n","          images = list(image.to(device) for image in images)\n","          targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","          # print(images)\n","\n","          loss_dict = model(images, targets) # returns losses and detections\n","          perf.add_score(loss_dict, training=True)\n","          print(\"Output model/Loss :\", loss_dict)\n","\n","          losses = sum(loss for loss in loss_dict.values())\n","          loss_value = losses.item()\n","\n","          epoch_loss += loss_value\n","\n","          optimizer.zero_grad()\n","          losses.backward()\n","          optimizer.step()\n","\n","          if itr % 50 == 0:\n","              print(f\"Iteration #{itr} loss: {loss_value}\")\n","              torch.save(model.state_dict(), ROOT_DIR_SAVING + SAVEDMODEL_NAME)\n","              perf.save(perf)\n","              print(\"SavedOnce\")\n","\n","          itr += 1\n","          iteration += 1\n","      \n","      # update the learning rate\n","      if lr_scheduler is not None:\n","          lr_scheduler.step()\n","\n","      print(f\"Epoch #{epoch} loss: {epoch_loss/iteration}\")\n","      MODEL_NAME = \"/fasterrcnn_resnet50fpn_SGD0005_None_Batch8_Epoch2.pth\"\n","      torch.save(model.state_dict(), ROOT_DIR_SAVING + MODEL_NAME)\n","      perf.save(perf)\n","      print(\"SaveAtEpoch\")\n","\n","  MODEL_NAME = \"/fasterrcnn_resnet50fpn_SGD0005_None_Batch8_Epoch2.pth\"\n","  torch.save(model.state_dict(), ROOT_DIR_SAVING + MODEL_NAME)\n","  perf.save(perf)\n","  print(\"Training is over.\")\n","  print(\"The model is saved.\")\n","\n","def test(args):\n","  return 0"],"metadata":{"id":"H4SrIPBsTRQu","executionInfo":{"status":"ok","timestamp":1645892409564,"user_tz":-60,"elapsed":209,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Settings\n","    parser = argparse.ArgumentParser(description='Point Cloud Part Segmentation')\n","    parser.add_argument('-f') #https://stackoverflow.com/questions/42249982/systemexit-2-error-when-calling-parse-args-within-ipython?noredirect=1&lq=1\n","    parser.add_argument('--eval', type=bool,  default=False, help='Evaluate the model')\n","    parser.add_argument('--model', type=str, default='fasterrcnn', metavar='N',\n","                        choices=['fasterrcnn'], help='Model to use')\n","    parser.add_argument('--batch_size', type=int, default=8, metavar='batch_size',\n","                        help='Size of batch)')\n","    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n","                        help='Number of episode to train ')\n","    parser.add_argument('--use_sgd', type=bool, default=True,\n","                        help='Use SGD')\n","    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n","                        help='learning rate (default: 0.001, 0.1 if using sgd)')\n","    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n","                        help='SGD momentum (default: 0.9)')\n","    parser.add_argument('--scheduler', type=str, default='cos', metavar='N',\n","                        choices=['cos', 'step'],\n","                        help='Scheduler to use, [cos, step]')\n","\n","    args = parser.parse_args()\n","\n","    if args.eval:\n","      training(args)\n","    else:\n","      test(args)\n","    "],"metadata":{"id":"eux2UMEzi8kd"},"execution_count":null,"outputs":[]}]}