{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FasterRCNN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1i9J2iFNbyJj-Z2c4DjQE6Nh6jchrSGsA","authorship_tag":"ABX9TyOnWbI53ik6x3+xl2FceITC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdRwPIDA0nWB","executionInfo":{"status":"ok","timestamp":1646496421928,"user_tz":-60,"elapsed":53621,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"e0891cf6-f73c-4bd2-add9-737f92a2f255"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","!pip install \"opencv-python-headless<4.3\"\n","import cv2\n","import xml.etree.ElementTree as ET\n","import pickle as cPickle\n","import argparse\n","\n","import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.rpn import AnchorGenerator\n","import torchvision.transforms as T\n","\n","import torch.nn as nn\n","import torch\n","\n","# !pip install albumentations==1.1.0\n","!pip install git+https://github.com/albumentations-team/albumentations.git\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import sys\n","sys.path.insert(0,\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/dataset\")\n","\n","# from engine import train_one_epoch, evaluate"],"metadata":{"id":"4KJo128n1INW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647184236775,"user_tz":-60,"elapsed":31363,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"885b9b77-f147-4b73-e2f1-1661b49fcc29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opencv-python-headless<4.3\n","  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n","\u001b[K     |████████████████████████████████| 21.6 MB 6.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.5)\n","Installing collected packages: opencv-python-headless\n","Successfully installed opencv-python-headless-4.2.0.34\n","Collecting git+https://github.com/albumentations-team/albumentations.git\n","  Cloning https://github.com/albumentations-team/albumentations.git to /tmp/pip-req-build-dk4cz1g8\n","  Running command git clone -q https://github.com/albumentations-team/albumentations.git /tmp/pip-req-build-dk4cz1g8\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (1.4.1)\n","Requirement already satisfied: scikit-image<0.19,>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (0.18.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (3.13)\n","Collecting qudida>=0.0.4\n","  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (4.1.2.30)\n","Requirement already satisfied: opencv-python-headless>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.1.0) (4.2.0.34)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.1.0) (3.10.0.2)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.1.0) (1.0.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (2.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (2.6.3)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (1.2.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (3.2.2)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (2021.11.2)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (7.1.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image<0.19,>=0.16.1->albumentations==1.1.0) (1.15.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.1.0) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.1.0) (1.1.0)\n","Building wheels for collected packages: albumentations\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-1.1.0-py3-none-any.whl size=112723 sha256=dbb6bbefd74362f6f82f157cb2114f8d36adea82b1dcd4dc15e5368a2013f537\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-f_c6_8fc/wheels/09/5b/e0/38cb85186b3cb979092395de69893163f9a223a72066123599\n","Successfully built albumentations\n","Installing collected packages: qudida, albumentations\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-1.1.0 qudida-0.0.4\n"]}]},{"cell_type":"code","source":["# Manage multiple versions of python with pip\n","# py -3.8 -m pip install package\n","#https://stackoverflow.com/questions/2812520/dealing-with-multiple-python-versions-and-pip\n","# Inspired by torchvision example: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n","\n","class BirdDataset(torch.utils.data.Dataset):\n","    \"\"\"Class to charecterize the bird dataset\"\"\"\n","\n","    def __init__(self, root_dir, transforms=None):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.root = root_dir\n","        self.transforms = transforms\n","        \n","        self.imgs = list(sorted(os.listdir(os.path.join(root_dir, \"all_images\")), key=lambda x: int(os.path.splitext(x)[0])))  # list of all image names - jpg\n","        print(self.imgs)\n","        self.boxes = list(sorted(os.listdir(os.path.join(root_dir, \"all_labels\")), key=lambda x: int(os.path.splitext(x)[0]))) # list of all image names - xml\n","    \n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Loads and returns a sample from the dataset at the given index idx\"\"\"\n","        # load images and boxes\n","        img_path = os.path.join(self.root, \"all_images\", self.imgs[idx])\n","        box_path = os.path.join(self.root, \"all_labels\", self.boxes[idx])\n","        # print(\"Image path\", img_path)\n","        # print(type(cv2.imread(img_path, cv2.IMREAD_COLOR)))\n","        # img = Image.open(img_path).convert(\"RGB\")\n","        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        img /= 255.0\n","        \n","        # get boxes for each bird\n","        document = ET.parse(box_path)\n","        root = document.getroot()\n","        boxes = []\n","        for item in root.findall(\".//object/bndbox\"):\n","            xmin = float(item.find('xmin').text)\n","            xmax = float(item.find('xmax').text)\n","            ymin = float(item.find('ymin').text)\n","            ymax = float(item.find('ymax').text)\n","\n","            box = [xmin, ymin, xmax, ymax]\n","            boxes.append(box)\n","        num_objs = len(boxes)\n","\n","        # convert everything into a torch.Tensor\n","        image_id = torch.tensor([idx+1])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.ones((num_objs,), dtype=torch.int64) # only one class : a bird\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        # target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        # target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            # img = self.transforms(img)\n","            sample = {\n","                'image': img,\n","                'bboxes': target['boxes'],\n","                'labels': labels\n","            }\n","            sample = self.transforms(**sample)\n","\n","            img = sample['image']\n","            if len(sample['bboxes']) == 0: # \n","                target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n","            else:\n","              target['boxes'] = torch.tensor(sample['bboxes'])\n","\n","        return img, target\n","\n","# class BirdSpeciesDataset():"],"metadata":{"id":"SIZlc4Ut1rTV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_model(trained=True, save_path=None):\n","  device = 'cpu'\n","  # model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","  anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n","  aspect_ratios = ((0.25, 0.5, 0.75, 1.0, 2.0, 3.0),) * len(anchor_sizes)\n","  anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","  # roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","  #                                               output_size=7,\n","  #                                               sampling_ratio=2)\n","\n","  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=True, rpn_anchor_generator=anchor_generator)\n","  num_classes = 2  # 1 class (bird) + background\n","\n","  # get number of input features for the classifier\n","  in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","  # replace the pre-trained head with a new one pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=None\n","  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","  model.to(device)\n","\n","  if trained:\n","    if save_path == None: print(\"No path to the saved model\")\n","    model.load_state_dict(torch.load(save_path))\n","\n","  return model\n","\n","# What is the difference between the YOLO version ?\n","# def get_YOLOv5model(trained=True, save_path=None):\n","#   return model"],"metadata":{"id":"do6oPKRF4jfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#########\n","# UTILS #\n","#########\n","def collate_fn(batch):\n","  \"\"\"Necessary for DataLoader\"\"\"\n","  return tuple(zip(*batch))\n","\n","def get_transform(train):\n","  \"\"\"Tansform the training and test set. Data Augmentation is made here.\"\"\"\n","  transforms = []\n","  # transforms.append(A.Resize(224, 224, interpolation = cv2.INTER_LANCZOS4))\n","  if train:\n","      transforms.append(A.Flip(0.5))\n","      transforms.append(A.ColorJitter())\n","      transforms.append(A.RandomCrop(width=1024, height=1024))\n","      # transforms.append(A.Normalize(mean=[0.5977, 0.5537, 0.5084], std=[0.0899, 0.0806, 0.0759]))           \n","  transforms.append(ToTensorV2(p=1.0)) \n","  return A.Compose(transforms, bbox_params={'format': 'pascal_voc', 'min_visibility': 0.5, 'label_fields': ['labels']})\n","\n","def save_performance(score, filename):\n","  \"\"\"Save the scores (4) on a txt file of name filename\"\"\"\n","  return 0\n","\n","class Performance():\n","    \"\"\"Class to calculate and store the performance/score of a model\"\"\"\n","    def __init__(self, root_save=None, args=None):\n","        \"\"\"\n","        Args:\n","            root_save (string): \n","            params (callable, optional):\n","        \"\"\"\n","        self.root_save = root_save\n","        self.args = args\n","        self.train_score = [[],[],[],[]]\n","        self.validation_score = [[],[],[],[]]\n","        self.training_iou = []\n","        self.test_iou = []\n","        self.iterperepoch = 0\n","        self.iterperepochval = 0\n","\n","    def add_score(self, values, training):\n","      \"\"\"\n","      Add the different score of the model for every iteration to the corresponding list of score\n","      values : dictionnary containing  'loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg'\n","      \"\"\"\n","      if training:\n","          self.train_score[0].append(values['loss_classifier'].cpu().detach().item())\n","          self.train_score[1].append(values['loss_box_reg'].cpu().detach().item())\n","          self.train_score[2].append(values['loss_objectness'].cpu().detach().item())\n","          self.train_score[3].append(values['loss_rpn_box_reg'].cpu().detach().item())\n","      else:    \n","          self.validation_score[0].append(values['loss_classifier'].cpu().detach().item())\n","          self.validation_score[1].append(values['loss_box_reg'].cpu().detach().item())\n","          self.validation_score[2].append(values['loss_objectness'].cpu().detach().item())\n","          self.validation_score[3].append(values['loss_rpn_box_reg'].cpu().detach().item())\n","    \n","    def add_accuracy(self, val, training):\n","        if training:\n","            self.training_iou.append(val)\n","        else:\n","            self.test_iou.append(val)\n","\n","    def save_class(self, obj):\n","        \"\"\"Save the class in txt file\"\"\"\n","        with open(self.root_save, 'wb') as outp:  # Overwrites any existing file.\n","            cPickle.dump(obj, outp, cPickle.HIGHEST_PROTOCOL)\n","\n","    def save_score(self, path):\n","        file = open(path, \"w+\")\n","        file.write(str(self.train_score))\n","        file.close()\n","\n","    def load(self):\n","        \"\"\"Load the class from txt file\"\"\"\n","        file = open(self.root_save,'rb')\n","        dataPickle = file.read()\n","        file.close()\n","        return cPickle.loads(dataPickle)\n","        # self.__dict__ = cPickle.loads(dataPickle)\n","\n","def intersect(box_a, box_b):\n","    \"\"\" https://github.com/amdegroot/ssd.pytorch/blob/master/layers/box_utils.py#L48\n","    We resize both tensors to [A,B,2] without new malloc:\n","    [A,2] -> [A,1,2] -> [A,B,2]\n","    [B,2] -> [1,B,2] -> [A,B,2]\n","    Then we compute the area of intersect between box_a and box_b.\n","    Args:\n","      box_a: (tensor) bounding boxes, Shape: [A,4].\n","      box_b: (tensor) bounding boxes, Shape: [B,4].\n","    Return:\n","      (tensor) intersection area, Shape: [A,B].\n","    \"\"\"\n","    A = box_a.size(0)\n","    B = box_b.size(0)\n","    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n","                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n","    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n","                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n","    inter = torch.clamp((max_xy - min_xy), min=0)\n","    return inter[:, :, 0] * inter[:, :, 1]\n","\n","\n","def jaccard(box_a, box_b):\n","    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n","    is simply the intersection over union of two boxes.  Here we operate on\n","    ground truth boxes and default boxes.\n","    E.g.:\n","        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n","    Args:\n","        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n","        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n","    Return:\n","        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n","    \"\"\"\n","    inter = intersect(box_a, box_b)\n","    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n","              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n","    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n","              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n","    union = area_a + area_b - inter\n","    return inter / union  # [A,B]\n","\n","def compute_ap(recall, precision):\n","    \"\"\" Compute the average precision, given the recall and precision curves.\n","    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n","    # Arguments\n","        recall:    The recall curve (list).\n","        precision: The precision curve (list).\n","    # Returns\n","        The average precision as computed in py-faster-rcnn.\n","    \"\"\"\n","    # correct AP calculation\n","    # first append sentinel values at the end\n","    mrec = np.concatenate(([0.0], recall, [1.0]))\n","    mpre = np.concatenate(([0.0], precision, [0.0]))\n","\n","    # compute the precision envelope\n","    for i in range(mpre.size - 1, 0, -1):\n","        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n","\n","    # to calculate area under PR curve, look for points\n","    # where X axis (recall) changes value\n","    i = np.where(mrec[1:] != mrec[:-1])[0]\n","\n","    # and sum (\\Delta recall) * prec\n","    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n","    return ap\n","\n","def batchaccuracy_score(gtboxes, predboxes):\n","  \"\"\" Caculate the accuracy of the predictions over an image based on IOU\n","      Iteration Over Union\n","      Args:\n","        gtboxes: (dict) Ground truth bounding boxes for all images of one batch\n","        predboxes: (dict) Predicted boxes for all images of one batch (Pytorch output)\n","  \"\"\"\n","  precision = []\n","  recall = []\n","  allboxstate = []\n","  allboxpredscore = []\n","  for i in range(len(gtboxes)): # per image\n","    # Filter the bounding boxes with confidence score < 0.5\n","    predscore = predboxes[i]['scores'].cpu().detach().numpy()\n","    scoremask = (predscore >= 0.5)\n","    predscore = predscore[scoremask]\n","    allboxpredscore += predscore\n","\n","    # Sort the bounding boxes, first one has highest confidence score\n","    # orderscoremask = predscore.argsort()[::-1][:len(predscore)]\n","    # predscore = predscore[orderscoremask]\n","\n","    # Calculate the IoU score between ground-truth and predictions\n","    # Check if the order compare to score stay the same\n","    iouscore = jaccard(gtboxes[i]['boxes'], predscore) # [num_obj, num_prediction]\n","    iouscore = iouscore.cpu().detach().numpy()\n","\n","    # 0=FalseNegative, 1=FalsePositive, 2=TruePositive\n","    for i in range(len(gtboxes[i]['boxes'])):\n","      # If there is a ground-truth but no prediction: full lign is 0\n","      if len(iouscore[i]==0) == len(iouscore[i]):\n","        allboxstate.append(0)\n","      TPs = len(iouscore[i] >= 0.5) # TPs for one ground-truth, keep only one other are FalsePositive.\n","      if TPs > 0:\n","        allboxstate.append(2)\n","      else:\n","        allboxstate += [1]*(TPs-1)\n","\n","      smalliou = iouscore[i][iouscore[i] < 0.5]\n","      FPs = len(smalliou > 0)\n","      allboxstate += [1]*(FPs)\n"],"metadata":{"id":"2ft0imD_SqZy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Global variable\n","ROOT_DIR_DATA = \"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/dataset\"\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/savedmodel\"\n","SAVEDMODEL_NAME = \"/fasterrcnn_SGD0005_Size224_Batch8_Epoch2.pth\"\n","SAVEDPERFORMANCE_NAME = \"/score_fasterrcnn_SGD0005_Size224_Batch8_Epoch2.pkl\"\n","\n","# instantiate dataset objects\n","ds = BirdDataset(ROOT_DIR_DATA, get_transform(train=True))\n","ds_test = BirdDataset(ROOT_DIR_DATA, get_transform(train=False))\n","\n","# set hyper-parameters\n","num_epochs = 1\n","num_classes = 2\n","num_coord = 4\n","num_workers = 2\n","batch_size = 8\n","\n","# instantiate data loaders\n","# split the dataset in train and test set\n","# random_seed = 1 # or any of your favorite number \n","# torch.manual_seed(random_seed)\n","indices = torch.randperm(len(ds)).tolist()\n","print(indices)\n","# torch.manual_seed(random_seed)\n","# indices = torch.randperm(len(ds)).tolist()\n","# print(indices)\n","# indices = torch.randperm(len(ds)).tolist()\n","# print(indices)\n","splitdataset = int(len(ds)*0.2)\n","dataset = torch.utils.data.Subset(ds, indices[:-splitdataset])\n","dataset_test = torch.utils.data.Subset(ds_test, indices[-splitdataset:])\n","\n","# define training and validation data loaders\n","data_loader_training = torch.utils.data.DataLoader(dataset, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, batch_size=batch_size)\n","data_loader_test = torch.utils.data.DataLoader(dataset_test, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, batch_size=batch_size)"],"metadata":{"id":"cm3-m2n_RgfT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647184392406,"user_tz":-60,"elapsed":434,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"4c8dfa6c-1159-4c2c-9b04-eb97980134cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['1.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '30.jpg', '31.jpg', '32.jpg', '33.jpg', '34.jpg', '35.jpg', '36.jpg', '37.jpg', '38.jpg', '39.jpg', '40.jpg', '41.jpg', '42.jpg', '43.jpg', '44.jpg', '45.jpg', '46.jpg', '47.jpg', '48.jpg', '49.jpg', '50.jpg', '51.jpg', '52.jpg', '53.jpg', '54.jpg', '55.jpg', '56.jpg', '57.jpg', '58.jpg', '59.jpg', '60.jpg', '61.jpg', '62.jpg', '63.jpg', '64.jpg', '65.jpg', '66.jpg', '67.jpg', '68.jpg', '69.jpg', '70.jpg', '71.jpg', '72.jpg', '73.jpg', '74.jpg', '75.jpg', '76.jpg', '77.jpg', '78.jpg', '79.jpg', '80.jpg', '81.jpg', '82.jpg', '83.jpg', '84.jpg', '85.jpg', '86.jpg', '87.jpg', '88.jpg', '89.jpg', '90.jpg', '91.jpg', '92.jpg', '93.jpg', '94.jpg', '95.jpg', '96.jpg', '97.jpg', '98.jpg', '99.jpg', '100.jpg', '101.jpg', '102.jpg', '103.jpg', '104.jpg', '105.jpg', '106.jpg', '107.jpg', '108.jpg', '109.jpg', '110.jpg', '111.jpg', '112.jpg', '113.jpg', '114.jpg', '115.jpg', '116.jpg', '117.jpg', '118.jpg', '119.jpg', '120.jpg', '121.jpg', '122.jpg', '123.jpg', '124.jpg', '125.jpg', '126.jpg', '127.jpg', '128.jpg', '129.jpg', '130.jpg', '131.jpg', '132.jpg', '133.jpg', '134.jpg', '135.jpg', '136.jpg', '137.jpg', '138.jpg', '139.jpg', '140.jpg', '141.jpg', '142.jpg', '143.jpg', '144.jpg', '145.jpg', '146.jpg', '147.jpg', '148.jpg', '149.jpg', '150.jpg', '151.jpg', '152.jpg', '153.jpg', '154.jpg', '155.jpg', '156.jpg', '157.jpg', '158.jpg', '159.jpg', '160.jpg', '161.jpg', '162.jpg', '163.jpg', '164.jpg', '165.jpg', '166.jpg', '167.jpg', '168.jpg', '169.jpg', '170.jpg', '171.jpg', '172.jpg', '173.jpg', '174.jpg', '175.jpg', '176.jpg', '177.jpg', '178.jpg', '179.jpg', '180.jpg', '181.jpg', '182.jpg', '183.jpg', '184.jpg', '185.jpg', '186.jpg', '187.jpg', '188.jpg', '189.jpg', '190.jpg', '191.jpg', '192.jpg', '193.jpg', '194.jpg', '195.jpg', '196.jpg', '197.jpg', '198.jpg', '199.jpg', '200.jpg', '201.jpg', '202.jpg', '203.jpg', '204.jpg', '205.jpg', '206.jpg', '207.jpg', '208.jpg', '209.jpg', '210.jpg', '211.jpg', '212.jpg', '213.jpg', '214.jpg', '215.jpg', '216.jpg', '217.jpg', '218.jpg', '219.jpg', '220.jpg', '221.jpg', '222.jpg', '223.jpg', '224.jpg', '225.jpg', '226.jpg', '227.jpg', '228.jpg', '229.jpg', '230.jpg', '231.jpg', '232.jpg', '233.jpg', '234.jpg', '235.jpg', '236.jpg', '237.jpg', '238.jpg', '239.jpg', '240.jpg', '241.jpg', '242.jpg', '243.jpg', '244.jpg', '245.jpg', '246.jpg', '247.jpg', '248.jpg', '249.jpg', '250.jpg', '251.jpg', '252.jpg', '253.jpg', '254.jpg', '255.jpg', '256.jpg', '257.jpg', '258.jpg', '259.jpg', '260.jpg', '261.jpg', '262.jpg', '263.jpg', '264.jpg', '265.jpg', '266.jpg', '267.jpg', '268.jpg', '269.jpg', '270.jpg', '271.jpg', '272.jpg', '273.jpg', '274.jpg', '275.jpg', '276.jpg', '277.jpg', '278.jpg', '279.jpg', '280.jpg', '281.jpg', '282.jpg', '283.jpg', '284.jpg', '285.jpg', '286.jpg', '287.jpg', '288.jpg', '289.jpg', '290.jpg', '291.jpg', '292.jpg', '293.jpg', '294.jpg', '295.jpg', '296.jpg', '297.jpg', '298.jpg', '299.jpg', '300.jpg', '301.jpg', '302.jpg', '303.jpg', '304.jpg', '305.jpg', '306.jpg', '307.jpg', '308.jpg', '309.jpg', '310.jpg', '311.jpg', '312.jpg', '313.jpg', '314.jpg', '315.jpg', '316.jpg', '317.jpg', '318.jpg', '319.jpg', '320.jpg', '321.jpg', '322.jpg', '323.jpg', '324.jpg', '325.jpg', '326.jpg', '327.jpg', '328.jpg', '329.jpg', '330.jpg', '331.jpg', '332.jpg', '333.jpg', '334.jpg', '335.jpg', '336.jpg', '337.jpg', '338.jpg', '339.jpg', '340.jpg', '341.jpg', '342.jpg', '343.jpg', '344.jpg', '345.jpg', '346.jpg', '347.jpg', '348.jpg', '349.jpg', '350.jpg', '351.jpg', '352.jpg', '353.jpg', '354.jpg', '355.jpg', '356.jpg', '357.jpg', '358.jpg', '359.jpg', '360.jpg', '361.jpg', '362.jpg', '363.jpg', '364.jpg', '365.jpg', '366.jpg', '367.jpg', '368.jpg', '369.jpg', '370.jpg', '371.jpg', '372.jpg', '373.jpg', '374.jpg', '375.jpg', '376.jpg', '377.jpg', '378.jpg', '379.jpg', '380.jpg', '381.jpg', '382.jpg', '383.jpg', '384.jpg', '385.jpg', '386.jpg', '387.jpg', '388.jpg', '389.jpg', '390.jpg', '391.jpg', '392.jpg', '393.jpg', '394.jpg', '395.jpg', '396.jpg', '397.jpg', '398.jpg', '399.jpg', '400.jpg', '401.jpg', '402.jpg', '403.jpg', '404.jpg', '405.jpg', '406.jpg', '407.jpg', '408.jpg', '409.jpg', '410.jpg', '411.jpg', '412.jpg', '413.jpg', '414.jpg', '415.jpg', '416.jpg', '417.jpg', '418.jpg', '419.jpg', '420.jpg', '421.jpg', '422.jpg', '423.jpg', '424.jpg', '425.jpg', '426.jpg', '427.jpg', '428.jpg', '429.jpg', '430.jpg', '431.jpg', '432.jpg', '433.jpg', '434.jpg', '435.jpg', '436.jpg', '437.jpg', '438.jpg', '439.jpg', '440.jpg', '441.jpg', '442.jpg', '443.jpg', '444.jpg', '445.jpg', '446.jpg', '447.jpg', '448.jpg', '449.jpg', '450.jpg', '451.jpg', '452.jpg', '453.jpg', '454.jpg', '455.jpg', '456.jpg', '457.jpg', '458.jpg', '459.jpg', '460.jpg', '461.jpg', '462.jpg', '463.jpg', '464.jpg', '465.jpg', '466.jpg', '467.jpg', '468.jpg', '469.jpg', '470.jpg', '471.jpg', '472.jpg', '473.jpg', '474.jpg', '475.jpg', '476.jpg', '477.jpg', '478.jpg', '479.jpg', '480.jpg', '481.jpg', '482.jpg', '483.jpg', '484.jpg', '485.jpg', '486.jpg', '487.jpg', '488.jpg', '489.jpg', '490.jpg', '491.jpg', '492.jpg', '493.jpg', '494.jpg', '495.jpg', '496.jpg', '497.jpg', '498.jpg', '499.jpg', '500.jpg', '501.jpg', '502.jpg', '503.jpg', '504.jpg', '505.jpg', '506.jpg', '507.jpg', '508.jpg', '509.jpg', '510.jpg', '511.jpg', '512.jpg', '513.jpg', '514.jpg', '515.jpg', '516.jpg', '517.jpg', '518.jpg', '519.jpg', '520.jpg', '521.jpg', '522.jpg', '523.jpg', '524.jpg', '525.jpg', '526.jpg', '527.jpg', '528.jpg', '529.jpg', '530.jpg', '531.jpg', '532.jpg', '533.jpg', '534.jpg', '535.jpg', '536.jpg', '537.jpg', '538.jpg', '539.jpg', '540.jpg', '541.jpg', '542.jpg', '543.jpg', '544.jpg', '545.jpg', '546.jpg', '547.jpg', '548.jpg', '549.jpg', '550.jpg', '551.jpg', '552.jpg', '553.jpg', '554.jpg', '555.jpg', '556.jpg', '557.jpg', '558.jpg', '559.jpg', '560.jpg', '561.jpg', '562.jpg', '563.jpg', '564.jpg', '565.jpg', '566.jpg', '567.jpg', '568.jpg', '569.jpg', '570.jpg', '571.jpg', '572.jpg', '573.jpg', '574.jpg', '575.jpg', '576.jpg', '577.jpg', '578.jpg', '579.jpg', '580.jpg', '581.jpg', '582.jpg', '583.jpg', '584.jpg', '585.jpg', '586.jpg', '587.jpg', '588.jpg', '589.jpg', '590.jpg', '591.jpg', '592.jpg', '593.jpg', '594.jpg', '595.jpg', '596.jpg', '597.jpg', '598.jpg', '599.jpg', '600.jpg', '601.jpg', '602.jpg', '603.jpg', '604.jpg', '605.jpg', '606.jpg', '607.jpg', '608.jpg', '609.jpg', '610.jpg', '611.jpg', '612.jpg', '613.jpg', '614.jpg', '615.jpg', '616.jpg', '617.jpg', '618.jpg', '619.jpg', '620.jpg', '621.jpg', '622.jpg', '623.jpg', '624.jpg', '625.jpg', '626.jpg', '627.jpg', '628.jpg', '629.jpg', '630.jpg', '631.jpg', '632.jpg', '633.jpg', '634.jpg', '635.jpg', '636.jpg', '637.jpg', '638.jpg', '639.jpg', '640.jpg', '641.jpg', '642.jpg', '643.jpg', '644.jpg', '645.jpg', '646.jpg', '647.jpg', '648.jpg', '649.jpg', '650.jpg', '651.jpg', '652.jpg', '653.jpg', '654.jpg', '655.jpg', '656.jpg']\n","['1.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '30.jpg', '31.jpg', '32.jpg', '33.jpg', '34.jpg', '35.jpg', '36.jpg', '37.jpg', '38.jpg', '39.jpg', '40.jpg', '41.jpg', '42.jpg', '43.jpg', '44.jpg', '45.jpg', '46.jpg', '47.jpg', '48.jpg', '49.jpg', '50.jpg', '51.jpg', '52.jpg', '53.jpg', '54.jpg', '55.jpg', '56.jpg', '57.jpg', '58.jpg', '59.jpg', '60.jpg', '61.jpg', '62.jpg', '63.jpg', '64.jpg', '65.jpg', '66.jpg', '67.jpg', '68.jpg', '69.jpg', '70.jpg', '71.jpg', '72.jpg', '73.jpg', '74.jpg', '75.jpg', '76.jpg', '77.jpg', '78.jpg', '79.jpg', '80.jpg', '81.jpg', '82.jpg', '83.jpg', '84.jpg', '85.jpg', '86.jpg', '87.jpg', '88.jpg', '89.jpg', '90.jpg', '91.jpg', '92.jpg', '93.jpg', '94.jpg', '95.jpg', '96.jpg', '97.jpg', '98.jpg', '99.jpg', '100.jpg', '101.jpg', '102.jpg', '103.jpg', '104.jpg', '105.jpg', '106.jpg', '107.jpg', '108.jpg', '109.jpg', '110.jpg', '111.jpg', '112.jpg', '113.jpg', '114.jpg', '115.jpg', '116.jpg', '117.jpg', '118.jpg', '119.jpg', '120.jpg', '121.jpg', '122.jpg', '123.jpg', '124.jpg', '125.jpg', '126.jpg', '127.jpg', '128.jpg', '129.jpg', '130.jpg', '131.jpg', '132.jpg', '133.jpg', '134.jpg', '135.jpg', '136.jpg', '137.jpg', '138.jpg', '139.jpg', '140.jpg', '141.jpg', '142.jpg', '143.jpg', '144.jpg', '145.jpg', '146.jpg', '147.jpg', '148.jpg', '149.jpg', '150.jpg', '151.jpg', '152.jpg', '153.jpg', '154.jpg', '155.jpg', '156.jpg', '157.jpg', '158.jpg', '159.jpg', '160.jpg', '161.jpg', '162.jpg', '163.jpg', '164.jpg', '165.jpg', '166.jpg', '167.jpg', '168.jpg', '169.jpg', '170.jpg', '171.jpg', '172.jpg', '173.jpg', '174.jpg', '175.jpg', '176.jpg', '177.jpg', '178.jpg', '179.jpg', '180.jpg', '181.jpg', '182.jpg', '183.jpg', '184.jpg', '185.jpg', '186.jpg', '187.jpg', '188.jpg', '189.jpg', '190.jpg', '191.jpg', '192.jpg', '193.jpg', '194.jpg', '195.jpg', '196.jpg', '197.jpg', '198.jpg', '199.jpg', '200.jpg', '201.jpg', '202.jpg', '203.jpg', '204.jpg', '205.jpg', '206.jpg', '207.jpg', '208.jpg', '209.jpg', '210.jpg', '211.jpg', '212.jpg', '213.jpg', '214.jpg', '215.jpg', '216.jpg', '217.jpg', '218.jpg', '219.jpg', '220.jpg', '221.jpg', '222.jpg', '223.jpg', '224.jpg', '225.jpg', '226.jpg', '227.jpg', '228.jpg', '229.jpg', '230.jpg', '231.jpg', '232.jpg', '233.jpg', '234.jpg', '235.jpg', '236.jpg', '237.jpg', '238.jpg', '239.jpg', '240.jpg', '241.jpg', '242.jpg', '243.jpg', '244.jpg', '245.jpg', '246.jpg', '247.jpg', '248.jpg', '249.jpg', '250.jpg', '251.jpg', '252.jpg', '253.jpg', '254.jpg', '255.jpg', '256.jpg', '257.jpg', '258.jpg', '259.jpg', '260.jpg', '261.jpg', '262.jpg', '263.jpg', '264.jpg', '265.jpg', '266.jpg', '267.jpg', '268.jpg', '269.jpg', '270.jpg', '271.jpg', '272.jpg', '273.jpg', '274.jpg', '275.jpg', '276.jpg', '277.jpg', '278.jpg', '279.jpg', '280.jpg', '281.jpg', '282.jpg', '283.jpg', '284.jpg', '285.jpg', '286.jpg', '287.jpg', '288.jpg', '289.jpg', '290.jpg', '291.jpg', '292.jpg', '293.jpg', '294.jpg', '295.jpg', '296.jpg', '297.jpg', '298.jpg', '299.jpg', '300.jpg', '301.jpg', '302.jpg', '303.jpg', '304.jpg', '305.jpg', '306.jpg', '307.jpg', '308.jpg', '309.jpg', '310.jpg', '311.jpg', '312.jpg', '313.jpg', '314.jpg', '315.jpg', '316.jpg', '317.jpg', '318.jpg', '319.jpg', '320.jpg', '321.jpg', '322.jpg', '323.jpg', '324.jpg', '325.jpg', '326.jpg', '327.jpg', '328.jpg', '329.jpg', '330.jpg', '331.jpg', '332.jpg', '333.jpg', '334.jpg', '335.jpg', '336.jpg', '337.jpg', '338.jpg', '339.jpg', '340.jpg', '341.jpg', '342.jpg', '343.jpg', '344.jpg', '345.jpg', '346.jpg', '347.jpg', '348.jpg', '349.jpg', '350.jpg', '351.jpg', '352.jpg', '353.jpg', '354.jpg', '355.jpg', '356.jpg', '357.jpg', '358.jpg', '359.jpg', '360.jpg', '361.jpg', '362.jpg', '363.jpg', '364.jpg', '365.jpg', '366.jpg', '367.jpg', '368.jpg', '369.jpg', '370.jpg', '371.jpg', '372.jpg', '373.jpg', '374.jpg', '375.jpg', '376.jpg', '377.jpg', '378.jpg', '379.jpg', '380.jpg', '381.jpg', '382.jpg', '383.jpg', '384.jpg', '385.jpg', '386.jpg', '387.jpg', '388.jpg', '389.jpg', '390.jpg', '391.jpg', '392.jpg', '393.jpg', '394.jpg', '395.jpg', '396.jpg', '397.jpg', '398.jpg', '399.jpg', '400.jpg', '401.jpg', '402.jpg', '403.jpg', '404.jpg', '405.jpg', '406.jpg', '407.jpg', '408.jpg', '409.jpg', '410.jpg', '411.jpg', '412.jpg', '413.jpg', '414.jpg', '415.jpg', '416.jpg', '417.jpg', '418.jpg', '419.jpg', '420.jpg', '421.jpg', '422.jpg', '423.jpg', '424.jpg', '425.jpg', '426.jpg', '427.jpg', '428.jpg', '429.jpg', '430.jpg', '431.jpg', '432.jpg', '433.jpg', '434.jpg', '435.jpg', '436.jpg', '437.jpg', '438.jpg', '439.jpg', '440.jpg', '441.jpg', '442.jpg', '443.jpg', '444.jpg', '445.jpg', '446.jpg', '447.jpg', '448.jpg', '449.jpg', '450.jpg', '451.jpg', '452.jpg', '453.jpg', '454.jpg', '455.jpg', '456.jpg', '457.jpg', '458.jpg', '459.jpg', '460.jpg', '461.jpg', '462.jpg', '463.jpg', '464.jpg', '465.jpg', '466.jpg', '467.jpg', '468.jpg', '469.jpg', '470.jpg', '471.jpg', '472.jpg', '473.jpg', '474.jpg', '475.jpg', '476.jpg', '477.jpg', '478.jpg', '479.jpg', '480.jpg', '481.jpg', '482.jpg', '483.jpg', '484.jpg', '485.jpg', '486.jpg', '487.jpg', '488.jpg', '489.jpg', '490.jpg', '491.jpg', '492.jpg', '493.jpg', '494.jpg', '495.jpg', '496.jpg', '497.jpg', '498.jpg', '499.jpg', '500.jpg', '501.jpg', '502.jpg', '503.jpg', '504.jpg', '505.jpg', '506.jpg', '507.jpg', '508.jpg', '509.jpg', '510.jpg', '511.jpg', '512.jpg', '513.jpg', '514.jpg', '515.jpg', '516.jpg', '517.jpg', '518.jpg', '519.jpg', '520.jpg', '521.jpg', '522.jpg', '523.jpg', '524.jpg', '525.jpg', '526.jpg', '527.jpg', '528.jpg', '529.jpg', '530.jpg', '531.jpg', '532.jpg', '533.jpg', '534.jpg', '535.jpg', '536.jpg', '537.jpg', '538.jpg', '539.jpg', '540.jpg', '541.jpg', '542.jpg', '543.jpg', '544.jpg', '545.jpg', '546.jpg', '547.jpg', '548.jpg', '549.jpg', '550.jpg', '551.jpg', '552.jpg', '553.jpg', '554.jpg', '555.jpg', '556.jpg', '557.jpg', '558.jpg', '559.jpg', '560.jpg', '561.jpg', '562.jpg', '563.jpg', '564.jpg', '565.jpg', '566.jpg', '567.jpg', '568.jpg', '569.jpg', '570.jpg', '571.jpg', '572.jpg', '573.jpg', '574.jpg', '575.jpg', '576.jpg', '577.jpg', '578.jpg', '579.jpg', '580.jpg', '581.jpg', '582.jpg', '583.jpg', '584.jpg', '585.jpg', '586.jpg', '587.jpg', '588.jpg', '589.jpg', '590.jpg', '591.jpg', '592.jpg', '593.jpg', '594.jpg', '595.jpg', '596.jpg', '597.jpg', '598.jpg', '599.jpg', '600.jpg', '601.jpg', '602.jpg', '603.jpg', '604.jpg', '605.jpg', '606.jpg', '607.jpg', '608.jpg', '609.jpg', '610.jpg', '611.jpg', '612.jpg', '613.jpg', '614.jpg', '615.jpg', '616.jpg', '617.jpg', '618.jpg', '619.jpg', '620.jpg', '621.jpg', '622.jpg', '623.jpg', '624.jpg', '625.jpg', '626.jpg', '627.jpg', '628.jpg', '629.jpg', '630.jpg', '631.jpg', '632.jpg', '633.jpg', '634.jpg', '635.jpg', '636.jpg', '637.jpg', '638.jpg', '639.jpg', '640.jpg', '641.jpg', '642.jpg', '643.jpg', '644.jpg', '645.jpg', '646.jpg', '647.jpg', '648.jpg', '649.jpg', '650.jpg', '651.jpg', '652.jpg', '653.jpg', '654.jpg', '655.jpg', '656.jpg']\n","[255, 283, 178, 402, 125, 423, 171, 62, 336, 347, 561, 101, 555, 583, 24, 31, 516, 503, 438, 118, 137, 245, 18, 598, 459, 133, 558, 593, 43, 272, 64, 489, 542, 324, 483, 258, 389, 525, 418, 243, 265, 603, 433, 257, 447, 630, 536, 386, 82, 538, 609, 393, 235, 244, 566, 388, 400, 504, 111, 528, 367, 450, 517, 351, 269, 49, 651, 288, 208, 623, 381, 338, 642, 560, 406, 303, 510, 322, 612, 445, 180, 364, 481, 298, 250, 621, 270, 53, 157, 518, 99, 397, 523, 145, 458, 349, 126, 176, 396, 592, 231, 328, 74, 268, 649, 552, 163, 316, 443, 645, 638, 496, 60, 201, 622, 632, 86, 411, 442, 394, 624, 425, 81, 387, 41, 70, 218, 332, 605, 577, 357, 531, 39, 85, 158, 348, 341, 144, 236, 162, 203, 77, 507, 580, 594, 143, 533, 292, 363, 543, 619, 334, 252, 102, 326, 65, 436, 600, 486, 278, 230, 151, 227, 524, 550, 279, 444, 190, 150, 27, 179, 589, 403, 63, 492, 373, 554, 251, 146, 112, 346, 114, 54, 181, 653, 35, 512, 284, 342, 131, 414, 317, 432, 23, 285, 104, 437, 121, 647, 207, 478, 287, 84, 454, 238, 185, 625, 3, 392, 225, 187, 511, 11, 416, 575, 369, 165, 614, 464, 491, 330, 371, 152, 6, 220, 379, 302, 117, 48, 570, 217, 72, 10, 539, 224, 51, 263, 650, 16, 613, 66, 67, 398, 318, 547, 482, 590, 329, 532, 562, 602, 479, 295, 7, 192, 282, 574, 549, 646, 559, 635, 616, 212, 395, 153, 610, 587, 654, 530, 427, 407, 299, 202, 300, 506, 261, 2, 434, 534, 474, 568, 584, 58, 501, 19, 110, 563, 122, 355, 159, 356, 100, 273, 206, 631, 541, 475, 267, 228, 306, 149, 473, 608, 604, 399, 173, 59, 350, 435, 597, 606, 209, 42, 643, 361, 229, 161, 502, 127, 242, 83, 579, 219, 460, 420, 428, 429, 0, 247, 239, 305, 591, 540, 515, 259, 553, 644, 246, 211, 197, 588, 103, 293, 487, 467, 415, 138, 572, 462, 156, 214, 544, 452, 69, 76, 497, 488, 108, 537, 75, 164, 294, 421, 73, 188, 307, 375, 321, 359, 417, 611, 455, 327, 582, 551, 410, 457, 196, 274, 168, 256, 80, 499, 172, 301, 215, 8, 424, 194, 469, 124, 109, 490, 370, 277, 32, 485, 529, 629, 15, 377, 314, 155, 365, 174, 130, 14, 576, 169, 466, 312, 129, 154, 468, 309, 340, 476, 13, 626, 376, 25, 12, 360, 5, 266, 276, 627, 311, 52, 461, 358, 20, 439, 652, 463, 385, 526, 29, 431, 47, 186, 97, 291, 175, 404, 55, 1, 509, 297, 222, 374, 237, 353, 233, 571, 304, 71, 37, 286, 446, 513, 639, 615, 136, 88, 498, 595, 451, 527, 40, 141, 30, 641, 308, 567, 38, 195, 520, 565, 405, 413, 453, 94, 343, 323, 320, 578, 198, 640, 249, 495, 205, 620, 4, 636, 573, 315, 419, 262, 95, 120, 135, 191, 449, 232, 585, 441, 505, 477, 456, 115, 494, 200, 105, 637, 92, 628, 148, 480, 132, 199, 337, 9, 290, 170, 107, 147, 607, 440, 390, 119, 248, 382, 535, 378, 333, 78, 226, 313, 296, 412, 140, 633, 408, 508, 183, 617, 61, 167, 569, 210, 557, 28, 142, 586, 96, 33, 281, 93, 254, 36, 519, 21, 213, 106, 384, 372, 470, 362, 134, 280, 484, 310, 422, 26, 89, 634, 193, 44, 368, 391, 91, 177, 98, 564, 240, 354, 548, 493, 79, 45, 234, 182, 216, 325, 184, 430, 380, 271, 426, 87, 521, 189, 17, 581, 139, 546, 331, 401, 260, 166, 345, 352, 601, 335, 275, 556, 465, 409, 241, 618, 57, 514, 500, 253, 448, 471, 204, 596, 46, 221, 366, 34, 648, 655, 383, 319, 223, 123, 599, 68, 522, 160, 344, 22, 289, 116, 472, 50, 128, 90, 339, 56, 264, 113, 545]\n"]}]},{"cell_type":"code","source":["#################\n","# Visualisation #\n","#################\n","device = \"cpu\"\n","images, targets = next(iter(data_loader_training))\n","# for images, targets in data_loader_training:\n","images = list(image.to(device) for image in images)\n","targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","# print(targets)\n","\n","for i in range(len(images)):\n","    # print(targets[i]['boxes'].size())\n","    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n","    image_id = targets[i]['image_id'].cpu().numpy().astype(np.int32)\n","    area = targets[i]['area'].cpu().numpy().astype(np.int32)\n","    print(image_id)\n","    # print(\"Area\", area)\n","    # print(images[i].size())\n","    sample = images[i].permute(1,2,0).cpu().numpy()\n","    # print(sample)\n","    # print(sample.shape)\n","    # print(\"Box\", boxes)\n","\n","    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","    sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n","    for box in boxes:\n","        cv2.rectangle(sample,\n","                    (int(box[0]), int(box[1])),\n","                    (int(box[2]), int(box[3])),\n","                    (1, 0, 0), 1)\n","        \n","    ax.imshow((sample * 255).astype(np.uint8))\n","    # plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/\"+str(image_id)+\"area\" + \".png\")\n","    plt.show()"],"metadata":{"id":"CN4ImOk-22lV","colab":{"base_uri":"https://localhost:8080/","height":796},"executionInfo":{"status":"error","timestamp":1647184397192,"user_tz":-60,"elapsed":1393,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"ce122106-7d73-41be-8ec8-50f44bae89d7"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-0eab6c3fda7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# for images, targets in data_loader_training:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\", line 363, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"<ipython-input-5-f7aff0f286d0>\", line 72, in __getitem__\n    sample = self.transforms(**sample)\n  File \"/usr/local/lib/python3.7/dist-packages/albumentations/core/composition.py\", line 205, in __call__\n    data = t(force_apply=force_apply, **data)\n  File \"/usr/local/lib/python3.7/dist-packages/albumentations/core/transforms_interface.py\", line 95, in __call__\n    return self.apply_with_params(params, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/albumentations/core/transforms_interface.py\", line 110, in apply_with_params\n    res[key] = target_function(arg, **dict(params, **target_dependencies))\n  File \"/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/crops/transforms.py\", line 47, in apply\n    return F.random_crop(img, self.height, self.width, h_start, w_start)\n  File \"/usr/local/lib/python3.7/dist-packages/albumentations/augmentations/crops/functional.py\", line 49, in random_crop\n    crop_height=crop_height, crop_width=crop_width, height=height, width=width\nValueError: Requested crop size (1024, 1024) is larger than the image size (600, 1024)\n"]}]},{"cell_type":"code","source":["#################################\n","# Training and Testing function #\n","#################################\n","def training(args):\n","# ds = BirdDataset(ROOT_DIR_DATA, get_transform(train=True))\n","  data_loader_training = torch.utils.data.DataLoader(dataset, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, batch_size=args.batch_size)\n","  data_loader_test = torch.utils.data.DataLoader(dataset_test, shuffle=True, collate_fn=collate_fn, num_workers=num_workers, batch_size=args.batch_size)\n","\n","  #device = \"cpu\"\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  model = get_model(trained=False)\n","\n","  params = [p for p in model.parameters() if p.requires_grad]\n","  optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","  # optimizer = torch.optim.Adam(params, lr=0.001)\n","  # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","  # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n","  lr_scheduler = None\n","\n","  itr = 1\n","  epoch_loss = 0\n","  perf = Performance(ROOT_DIR_SAVING + SAVEDPERFORMANCE_NAME, args)\n","\n","  for epoch in range(args.epochs):\n","      epoch_loss = 0\n","      iteration = 0\n","      model.train()\n","\n","      # Training\n","      for images, targets in data_loader_training:\n","          images = list(image.to(device) for image in images)\n","          targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","          print(targets)\n","\n","          loss_dict = model(images, targets) # returns losses and detections\n","          perf.add_score(loss_dict, training=True)\n","          print(\"Iter \",str(itr),\" - Output model/Loss :\", loss_dict)\n","\n","          losses = sum(loss for loss in loss_dict.values())\n","          loss_value = losses.item()\n","\n","          epoch_loss += loss_value\n","\n","          optimizer.zero_grad()\n","          losses.backward()\n","          optimizer.step()\n","\n","          if itr % 50 == 0:\n","              print(\"Iteration\" + str(itr) + \"loss: \" + str(loss_value))\n","              perf.save_class(perf)\n","              torch.save(model.state_dict(), ROOT_DIR_SAVING + SAVEDMODEL_NAME)         \n","              print(\"SavedOnce\")\n","\n","          itr += 1\n","          iteration += 1\n","\n","      # Validation\n","      itr_val = 0\n","      for images, targets in data_loader_test:\n","          images = list(image.to(device) for image in images)\n","          targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","          # print(images)\n","          with torch.no_grad():\n","              loss_dict = model(images, targets) # returns losses and detections\n","              itr_val += 1\n","              perf.add_score(loss_dict, training=False)\n","              print(\"Validation Loss :\", loss_dict)\n","      \n","      # Update the learning rate\n","      if lr_scheduler is not None:\n","          lr_scheduler.step()\n","\n","      print(\"Epoch \"+ str(epoch) + \"loss: \"+ str(epoch_loss/iteration))\n","      torch.save(model.state_dict(), ROOT_DIR_SAVING + SAVEDMODEL_NAME)\n","      if perf.iterperepoch == 0:\n","          perf.iterperepoch = itr\n","          perf.save_class(perf)\n","          print(\"SaveAtEpoch\")\n","      if perf.iterperepochval == 0:\n","        perf.iterperepoch = itr_val\n","      # result = train_one_epoch(model, optimizer, data_loader_training, device, epoch, print_freq=10)\n","\n","  torch.save(model.state_dict(), ROOT_DIR_SAVING + SAVEDMODEL_NAME)\n","  perf.save_class(perf)\n","  print(\"Training is over.\")\n","  print(\"The model is saved.\")\n","\n","def test(args):\n","  return 0"],"metadata":{"id":"H4SrIPBsTRQu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Settings\n","    parser = argparse.ArgumentParser(description='Bird Detection')\n","    parser.add_argument('-f') #https://stackoverflow.com/questions/42249982/systemexit-2-error-when-calling-parse-args-within-ipython?noredirect=1&lq=1\n","    parser.add_argument('--eval', type=bool,  default=False, help='Evaluate the model')\n","    parser.add_argument('--model', type=str, default='fasterrcnn', metavar='N',\n","                        choices=['fasterrcnn'], help='Model to use')\n","    parser.add_argument('--batch_size', type=int, default=8, metavar='batch_size',\n","                        help='Size of batch)')\n","    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n","                        help='Number of episode to train ')\n","    parser.add_argument('--use_sgd', type=bool, default=True,\n","                        help='Use SGD')\n","    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n","                        help='learning rate (default: 0.001, 0.1 if using sgd)')\n","    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n","                        help='SGD momentum (default: 0.9)')\n","    parser.add_argument('--scheduler', type=str, default='cos', metavar='N',\n","                        choices=['cos', 'step'],\n","                        help='Scheduler to use, [cos, step]')\n","\n","    args = parser.parse_args()\n","\n","    if not args.eval:\n","      training(args)\n","    else:\n","      test(args)\n","    "],"metadata":{"id":"eux2UMEzi8kd","colab":{"base_uri":"https://localhost:8080/","height":470},"outputId":"5e2598e2-f2be-4248-e5ec-443c1f0ac511","executionInfo":{"status":"error","timestamp":1647109802415,"user_tz":-60,"elapsed":2237,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'boxes': tensor([]), 'labels': tensor([1, 1, 1, 1, 1]), 'image_id': tensor([651]), 'area': tensor([84357., 96000., 52355., 23652., 30179.])}, {'boxes': tensor([]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1]), 'image_id': tensor([141]), 'area': tensor([31104., 28305., 33972., 36340., 65824., 24300., 40446.])}, {'boxes': tensor([]), 'labels': tensor([1, 1, 1, 1, 1, 1]), 'image_id': tensor([288]), 'area': tensor([ 6426.,  7772., 10965.,  7482.,  9928.,  8418.])}, {'boxes': tensor([]), 'labels': tensor([1, 1]), 'image_id': tensor([224]), 'area': tensor([ 99603., 100084.])}, {'boxes': tensor([]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'image_id': tensor([624]), 'area': tensor([6592., 5883., 2262., 5888., 4176., 4902., 6956., 3663., 2821., 4779.,\n","        7524., 4929., 3384., 3650.])}, {'boxes': tensor([]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'image_id': tensor([450]), 'area': tensor([6716., 4292., 4224., 4816., 5412., 6300., 4200., 5467., 5135., 4002.,\n","        6900., 3283.])}, {'boxes': tensor([]), 'labels': tensor([1, 1, 1, 1, 1, 1]), 'image_id': tensor([183]), 'area': tensor([47061., 27642., 25403., 28044., 30366., 35525.])}, {'boxes': tensor([]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]), 'image_id': tensor([569]), 'area': tensor([ 9790., 10816.,  5808.,  9100.,  5848., 10349., 15444.,  6549.,  6630.])}]\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-b0edf8b692d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-4f3a9f0e14af>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m           \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns losses and detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m           \u001b[0mperf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iter \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" - Output model/Loss :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     64\u001b[0m                         raise ValueError(\"Expected target boxes to be a tensor\"\n\u001b[1;32m     65\u001b[0m                                          \"of shape [N, 4], got {:}.\".format(\n\u001b[0;32m---> 66\u001b[0;31m                                              boxes.shape))\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     raise ValueError(\"Expected target boxes to be of type \"\n","\u001b[0;31mValueError\u001b[0m: Expected target boxes to be a tensorof shape [N, 4], got torch.Size([0])."]}]},{"cell_type":"code","source":["!pip3 install pickle5\n","import pickle5 as cPickle"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xpTseRaN6oC4","executionInfo":{"status":"ok","timestamp":1646927486885,"user_tz":-60,"elapsed":3920,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"a092a0fa-dd80-41bc-d03c-881959da0ee1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pickle5\n","  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n","\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 61 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 92 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 102 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 112 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 122 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 133 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 143 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 153 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 163 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 174 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 184 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 194 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 204 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 215 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 225 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 235 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 245 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 7.9 MB/s \n","\u001b[?25hInstalling collected packages: pickle5\n","Successfully installed pickle5-0.0.12\n"]}]},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/Thesis/savedmodel/score_fasterrcnn_SGD0005_SchedulerNone_Size576_Batch8_Epoch50_8020_anchor.pkl\"\n","perf1 = Performance(root_save=path)\n","perf1 = perf1.load()\n","# Plot the score of training and validation dataset of Faster RCNN\n","# \n","print(perf1.iterperepoch)\n","print(perf1.iterperepochval)\n","print(len(perf1.train_score[0]))\n","print(len(perf1.validation_score[0]))\n","print(perf1.args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJNbxWZO2h7A","executionInfo":{"status":"ok","timestamp":1646927532534,"user_tz":-60,"elapsed":1117,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"7e48e6cc-a802-4ca1-abc8-fdf29fa297aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["67\n","17\n","3300\n","850\n","Namespace(batch_size=8, epochs=50, eval=False, f=None, lr=0.001, model='fasterrcnn', momentum=0.9, scheduler='None', use_sgd=True)\n"]}]},{"cell_type":"code","source":["train_score = perf1.train_score\n","val_score = perf1.validation_score\n","\n","# Mean for epoch of training score\n","sizepoch = perf1.iterperepoch-1\n","epochscore1 = []\n","sum = 0\n","for i in range(len(train_score[0])):\n","  sum += train_score[0][i]\n","  if ((i+1) % sizepoch) == 0:\n","    epochscore1.append(sum/sizepoch)\n","    sum = 0\n","epochscore2 = []\n","sum = 0\n","for i in range(len(train_score[0])):\n","  sum += train_score[1][i]\n","  if ((i+1) % sizepoch) == 0:\n","    epochscore2.append(sum/sizepoch)\n","    sum = 0\n","epochscore3 = []\n","sum = 0\n","for i in range(len(train_score[0])):\n","  sum += train_score[2][i]\n","  if ((i+1) % sizepoch) == 0:\n","    epochscore3.append(sum/sizepoch)\n","    sum = 0\n","epochscore4 = []\n","sum = 0\n","for i in range(len(train_score[0])):\n","  sum += train_score[3][i]\n","  if ((i+1) % sizepoch) == 0:\n","    epochscore4.append(sum/sizepoch)\n","    sum = 0\n","\n","# Mean for epoch of validation score\n","sizepoch = perf1.iterperepochval\n","epochvalscore1 = []\n","sum = 0\n","for i in range(len(val_score[0])):\n","  sum += val_score[0][i]\n","  if ((i+1) % sizepoch) == 0:\n","    epochvalscore1.append(sum/sizepoch)\n","    sum = 0\n","epochvalscore2 = []\n","sum = 0\n","for i in range(len(val_score[0])):\n","  sum += val_score[1][i]\n","  if ((i+1) % sizepoch) == 0:\n","    epochvalscore2.append(sum/sizepoch)\n","    sum = 0\n","epochvalscore3 = []\n","sum = 0\n","for i in range(len(val_score[0])):\n","  sum += val_score[2][i]\n","  if ((i+1) % sizepoch) == 0:\n","    epochvalscore3.append(sum/sizepoch)\n","    sum = 0\n","epochvalscore4 = []\n","sum = 0\n","for i in range(len(val_score[0])):\n","  sum += val_score[3][i]\n","  if ((i+1) % sizepoch) == 0:\n","    epochvalscore4.append(sum/sizepoch)\n","    sum = 0\n","\n","print(len(epochscore1))\n","\n","fig, axs = plt.subplots(nrows=2,ncols=2, figsize=(15, 6))\n","# fig, axs = plt.subplots(nrows=2,ncols=3, sharex=True, sharey=True)\n","fig.suptitle('Score')\n","# fig.tight_layout()\n","# peut-être mieux: plt.subplots_adjust\n","\n","X = range(len(epochscore1)) + np.ones(len(epochscore1))\n","Xval = range(len(epochvalscore1))+ np.ones(len(epochscore1))\n","\n","axs[0,0].plot(X, epochscore1, label=\"Training\", color='C0')\n","axs[0,0].scatter(X, epochscore1, color='C0')\n","axs[0,0].plot(Xval, epochvalscore1, color='C1', label=\"Validation\")\n","axs[0,0].scatter(Xval, epochvalscore1, color='C1')\n","#axs[0,0].set_title('Sample 1')\n","axs[0,0].set_ylabel('Loss classifier')\n","# axs[0,0].set_xlabel('x1')\n","axs[0,0].legend()\n","\n","axs[0,1].plot(X, epochscore2, color='C0', label=\"Training\")\n","axs[0,1].scatter(X, epochscore2, color='C0')\n","axs[0,1].plot(Xval, epochvalscore2, color='C1', label=\"Validation\")\n","axs[0,1].scatter(Xval, epochvalscore2, color='C1')\n","#axs[0,1].set_title('Sample 2')\n","axs[0,1].set_ylabel('Loss box reg')\n","axs[0,1].legend()\n","#axs[0,1].set_xlabel('x2')\n","\n","# axs[1,0].scatter(X, train_score[2], color = \"blue\", edgecolors = \"white\", linewidths = 0.1, s=20, alpha = 0.7)\n","axs[1,0].plot(X, epochscore3, color='C0', label=\"Training\")\n","axs[1,0].scatter(X, epochscore3, color='C0')\n","axs[1,0].plot(Xval, epochvalscore3, color='C1', label=\"Validation\")\n","axs[1,0].scatter(Xval, epochvalscore3, color='C1')\n","#axs[1,0].set_title('Sample 3')\n","axs[1,0].set_ylabel('Loss objectness')\n","axs[1,0].set_xlabel('Epoch')\n","axs[1,0].legend()\n","\n","axs[1,1].plot(X, epochscore4, color='C0', label=\"Training\")\n","axs[1,1].scatter(X, epochscore4, color='C0')\n","axs[1,1].plot(Xval, epochvalscore4, color='C1', label=\"Validation\")\n","axs[1,1].scatter(Xval, epochvalscore4, color='C1')\n","#axs[1,1].set_title('Sample 4')\n","axs[1,1].set_ylabel('Loss rpn_box_reg')\n","axs[1,1].set_xlabel('Epoch')\n","axs[1,1].legend()\n","\n","for subg in axs.flat: # Apply settings on all subgraphs\n","    subg.set_facecolor('white') # remove grey background\n","\n","    subg.spines['right'].set_visible(False) # Hide the right and top spines\n","    subg.spines['top'].set_visible(False) \n","    subg.yaxis.set_ticks_position('left') # only show left and bottom axis\n","    subg.xaxis.set_ticks_position('bottom')\n","\n","plt.rcParams[\"figure.figsize\"] = (40,20) # remove to see overlapping subplots\n","# plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/performance/score_Batch8.png\")\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"0UjmbnWj1x7E","executionInfo":{"status":"error","timestamp":1646988478884,"user_tz":-60,"elapsed":683,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"b97ae3c9-396c-4c52-8f63-1136749d64a0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-b95fbaf4034b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Mean for epoch of training score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msizepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterperepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'perf1' is not defined"]}]},{"cell_type":"code","source":["path1 = \"/content/drive/MyDrive/Thesis/savedmodel/WithCOCO/score_fasterrcnn_SGD0005_SchedulerNone_Size224_Batch8_Epoch25_8020.pkl\"\n","perf1 = Performance(root_save=path1)\n","perf1 = perf1.load()\n","# path1 = \"/content/drive/MyDrive/Thesis/savedmodel/score_fasterrcnn_SGD0005_SchedulerNone_Size576_Batch8_Epoch50_8020_anchor.pkl\"\n","# perf1 = Performance(root_save=path1)\n","# perf1 = perf1.load()\n","path2 = \"/content/drive/MyDrive/Thesis/savedmodel/score_fasterrcnn_SGD0005_SchedulerNone_Size576_Batch8_Epoch50_8020.pkl\"\n","perf2 = Performance(root_save=path2)\n","perf2 = perf2.load()\n","\n","\n","fig, axs = plt.subplots(nrows=2,ncols=2, figsize=(15, 6))\n","# fig, axs = plt.subplots(nrows=2,ncols=3, sharex=True, sharey=True)\n","fig.suptitle('Score')\n","# fig.tight_layout()\n","# peut-être mieux: plt.subplots_adjust\n","\n","def add_performance(perf, color):\n","  train_score = perf.train_score\n","  val_score = perf.validation_score\n","\n","  # Mean for epoch of training score\n","  sizepoch = perf.iterperepoch-1\n","  epochscore1 = []\n","  sum = 0\n","  for i in range(len(train_score[0])):\n","    sum += train_score[0][i]\n","    if ((i+1) % sizepoch) == 0:\n","      epochscore1.append(sum/sizepoch)\n","      sum = 0\n","  epochscore2 = []\n","  sum = 0\n","  for i in range(len(train_score[0])):\n","    sum += train_score[1][i]\n","    if ((i+1) % sizepoch) == 0:\n","      epochscore2.append(sum/sizepoch)\n","      sum = 0\n","  epochscore3 = []\n","  sum = 0\n","  for i in range(len(train_score[0])):\n","    sum += train_score[2][i]\n","    if ((i+1) % sizepoch) == 0:\n","      epochscore3.append(sum/sizepoch)\n","      sum = 0\n","  epochscore4 = []\n","  sum = 0\n","  for i in range(len(train_score[0])):\n","    sum += train_score[3][i]\n","    if ((i+1) % sizepoch) == 0:\n","      epochscore4.append(sum/sizepoch)\n","      sum = 0\n","\n","  # Mean for epoch of validation score\n","  sizepoch = perf.iterperepochval\n","  epochvalscore1 = []\n","  sum = 0\n","  for i in range(len(val_score[0])):\n","    sum += val_score[0][i]\n","    if ((i+1) % sizepoch) == 0:\n","      epochvalscore1.append(sum/sizepoch)\n","      sum = 0\n","  epochvalscore2 = []\n","  sum = 0\n","  for i in range(len(val_score[0])):\n","    sum += val_score[1][i]\n","    if ((i+1) % sizepoch) == 0:\n","      epochvalscore2.append(sum/sizepoch)\n","      sum = 0\n","  epochvalscore3 = []\n","  sum = 0\n","  for i in range(len(val_score[0])):\n","    sum += val_score[2][i]\n","    if ((i+1) % sizepoch) == 0:\n","      epochvalscore3.append(sum/sizepoch)\n","      sum = 0\n","  epochvalscore4 = []\n","  sum = 0\n","  for i in range(len(val_score[0])):\n","    sum += val_score[3][i]\n","    if ((i+1) % sizepoch) == 0:\n","      epochvalscore4.append(sum/sizepoch)\n","      sum = 0\n","\n","  print(len(epochscore1))\n","\n","  X = range(len(epochscore1)) + np.ones(len(epochscore1))\n","  Xval = range(len(epochvalscore1))+ np.ones(len(epochscore1))\n","\n","  axs[0,0].plot(X, epochscore1, label=\"Training\", color=color[0])\n","  axs[0,0].scatter(X, epochscore1, color=color[0])\n","  axs[0,0].plot(Xval, epochvalscore1, color=color[1], label=\"Validation\")\n","  axs[0,0].scatter(Xval, epochvalscore1, color=color[1])\n","\n","  axs[0,1].plot(X, epochscore2, color=color[0], label=\"Training\")\n","  axs[0,1].scatter(X, epochscore2, color=color[0])\n","  axs[0,1].plot(Xval, epochvalscore2, color=color[1], label=\"Validation\")\n","  axs[0,1].scatter(Xval, epochvalscore2, color=color[1])\n","  #axs[0,1].set_xlabel('x2')\n","\n","  # axs[1,0].scatter(X, train_score[2], color = \"blue\", edgecolors = \"white\", linewidths = 0.1, s=20, alpha = 0.7)\n","  axs[1,0].plot(X, epochscore3, color=color[0], label=\"Training\")\n","  axs[1,0].scatter(X, epochscore3, color=color[0])\n","  axs[1,0].plot(Xval, epochvalscore3, color=color[1], label=\"Validation\")\n","  axs[1,0].scatter(Xval, epochvalscore3, color=color[1])\n","  #axs[1,0].set_title('Sample 3')\n","\n","  axs[1,1].plot(X, epochscore4, color=color[0], label=\"Training\")\n","  axs[1,1].scatter(X, epochscore4, color=color[0])\n","  axs[1,1].plot(Xval, epochvalscore4, color=color[1], label=\"Validation\")\n","  axs[1,1].scatter(Xval, epochvalscore4, color=color[1])\n","  #axs[1,1].set_title('Sample 4')\n","\n","add_performance(perf1, ['C0', 'C1'])\n","add_performance(perf2, ['C2', 'C3'])\n","\n","#axs[0,0].set_title('Sample 1')\n","axs[0,0].set_ylabel('Loss classifier')\n","# axs[0,0].set_xlabel('x1')\n","axs[0,0].legend()\n","#axs[0,1].set_title('Sample 2')\n","axs[0,1].set_ylabel('Loss box reg')\n","axs[0,1].legend()\n","axs[1,0].set_ylabel('Loss objectness')\n","axs[1,0].set_xlabel('Epoch')\n","axs[1,0].legend()\n","axs[1,1].set_ylabel('Loss rpn_box_reg')\n","axs[1,1].set_xlabel('Epoch')\n","axs[1,1].legend()"],"metadata":{"id":"0DW35prr6K4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install CocoDataset==0.1.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CkOvm6xSqtvS","executionInfo":{"status":"ok","timestamp":1647249575318,"user_tz":-60,"elapsed":4239,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"50944e0f-1268-4eb4-cd22-2aea01f0358f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting CocoDataset==0.1.2\n","  Downloading CocoDataset-0.1.2-py3-none-any.whl (4.2 kB)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from CocoDataset==0.1.2) (2.0.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pycocotools->CocoDataset==0.1.2) (1.21.5)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools->CocoDataset==0.1.2) (3.2.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->CocoDataset==0.1.2) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->CocoDataset==0.1.2) (3.0.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->CocoDataset==0.1.2) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->CocoDataset==0.1.2) (1.3.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools->CocoDataset==0.1.2) (1.15.0)\n","Installing collected packages: CocoDataset\n","Successfully installed CocoDataset-0.1.2\n"]}]},{"cell_type":"code","source":["!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n","# !wget --spider --recursive http://images.cocodataset.org\n","!unzip /content/annotations_trainval2017.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ThYyH_eArgxo","executionInfo":{"status":"ok","timestamp":1647250746462,"user_tz":-60,"elapsed":12791,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"88491120-02ca-43f2-a618-23466cf962ae"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-03-14 09:38:52--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n","Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.200.67\n","Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.200.67|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 252907541 (241M) [application/zip]\n","Saving to: ‘annotations_trainval2017.zip’\n","\n","annotations_trainva 100%[===================>] 241.19M  81.0MB/s    in 3.0s    \n","\n","2022-03-14 09:38:56 (81.0 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n","\n","Archive:  /content/annotations_trainval2017.zip\n","  inflating: annotations/instances_train2017.json  \n","  inflating: annotations/instances_val2017.json  \n","  inflating: annotations/captions_train2017.json  \n","  inflating: annotations/captions_val2017.json  \n","  inflating: annotations/person_keypoints_train2017.json  \n","  inflating: annotations/person_keypoints_val2017.json  \n"]}]},{"cell_type":"code","source":["from coco_dataset import coco_dataset_download as cocod\n","class_name='person'  #class name example \n","images_count=42       #count of images  \n","annotations_path='/content/annotations/instances_val2017.json' #path of coco dataset annotations \n","#call download function \n","cocod.coco_dataset_download(class_name,images_count,annotations_path)"],"metadata":{"id":"rMbkotn2umyR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","shutil.rmtree('/content/annotations')"],"metadata":{"id":"BKN2c3GNvW2g","executionInfo":{"status":"ok","timestamp":1647253267711,"user_tz":-60,"elapsed":256,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":34,"outputs":[]}]}