{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FasterRCNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNC0YcVeiFUCasK6vAHlteP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdRwPIDA0nWB","executionInfo":{"status":"ok","timestamp":1645724510431,"user_tz":-60,"elapsed":18698,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"ed084c7a-b121-4bda-b28b-8d3674b0d0a3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","import pickle as cPickle\n","\n","import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.rpn import AnchorGenerator\n","import torchvision.transforms as T\n","\n","import torch.nn as nn\n","import torch\n","\n","!pip install albumentations==0.4.6\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import sys\n","sys.path.insert(0,\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/dataset\")"],"metadata":{"id":"4KJo128n1INW","executionInfo":{"status":"ok","timestamp":1645725154900,"user_tz":-60,"elapsed":2,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Manage multiple versions of python with pip\n","# py -3.8 -m pip install package\n","#https://stackoverflow.com/questions/2812520/dealing-with-multiple-python-versions-and-pip\n","# Inspired by torchvision example: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n","\n","class BirdDataset(torch.utils.data.Dataset):\n","    \"\"\"Class to charecterize the bird dataset\"\"\"\n","\n","    def __init__(self, root_dir, transforms=None):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.root = root_dir\n","        self.transforms = transforms\n","        \n","        self.imgs = list(sorted(os.listdir(os.path.join(root_dir, \"all_images\"))))  # list of all image names - jpg\n","        self.boxes = list(sorted(os.listdir(os.path.join(root_dir, \"all_labels\")))) # list of all image names - xml\n","    \n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Loads and returns a sample from the dataset at the given index idx\"\"\"\n","        # load images and boxes\n","        img_path = os.path.join(self.root, \"all_images\", self.imgs[idx])\n","        box_path = os.path.join(self.root, \"all_labels\", self.boxes[idx])\n","        # print(\"Image path\", img_path)\n","        # print(type(cv2.imread(img_path, cv2.IMREAD_COLOR)))\n","        # img = Image.open(img_path).convert(\"RGB\")\n","        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        img /= 255.0\n","        \n","        # get boxes for each bird\n","        document = ET.parse(box_path)\n","        root = document.getroot()\n","        boxes = []\n","        for item in root.findall(\".//object/bndbox\"):\n","            xmin = float(item.find('xmin').text)\n","            xmax = float(item.find('xmax').text)\n","            ymin = float(item.find('ymin').text)\n","            ymax = float(item.find('ymax').text)\n","\n","            box = [xmin, ymin, xmax, ymax]\n","            boxes.append(box)\n","        num_objs = len(boxes)\n","\n","        # convert everything into a torch.Tensor\n","        image_id = torch.tensor([idx])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.ones((num_objs,), dtype=torch.int64) # only one class : a bird\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        # target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        # target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            # img = self.transforms(img)\n","            sample = {\n","                'image': img,\n","                'bboxes': target['boxes'],\n","                'labels': labels\n","            }\n","            sample = self.transforms(**sample)\n","\n","            img = sample['image']\n","            if len(sample['bboxes']) == 0: # \n","                target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n","            else:\n","                target['boxes'] = torch.tensor(sample['bboxes'])\n","\n","        return img, target"],"metadata":{"id":"SIZlc4Ut1rTV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_model(trained=True, save_path=None):\n","  device = 'cpu'\n","  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","  num_classes = 2  # 1 class (bird) + background\n","\n","  # get number of input features for the classifier\n","  in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","  # replace the pre-trained head with a new one\n","  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","  model.to(device)\n","\n","  if trained:\n","    if save_path == None: print(\"No path tto the saved model\")\n","    model.load_state_dict(torch.load(save_path))\n","\n","  return model"],"metadata":{"id":"do6oPKRF4jfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# UTILS #\n","#########\n","def collate_fn(batch):\n","  \"\"\"Necessary for DataLoader\"\"\"\n","  return tuple(zip(*batch))\n","\n","def get_transform(train):\n","  \"\"\"Tansform the training and test set. Data Augmentation is made here.\"\"\"\n","  transforms = []\n","  transforms.append(A.Resize(224, 224))\n","  if train:\n","      # transforms.append(A.RandomCrop(width=576, height=576))\n","      transforms.append(A.Flip(0.5))\n","      # transforms.append(A.Normalize(mean=[0.598, 0.554, 0.508], std=[0.090, 0.081, 0.076]))           \n","  transforms.append(ToTensorV2(p=1.0)) \n","  return A.Compose(transforms, bbox_params={'format': 'pascal_voc', 'min_visibility': 0.6, 'label_fields': ['labels']})\n"],"metadata":{"id":"2ft0imD_SqZy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Global variable\n","ROOT_DIR_DATA = \"/content/drive/MyDrive/Thesis/dataset\"\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/savedmodel\"\n","SAVEDMODEL_NAME = \"/fasterrcnn_resnet50fpn_SGD0005_None_Batch8_Epoch2.pth\"\n","\n","# instantiate dataset objects\n","ds = BirdDataset(ROOT_DIR_DATA, get_transform(train=True))\n","ds_test = BirdDataset(ROOT_DIR_DATA, get_transform(train=False))\n","\n","# set hyper-parameters\n","params = {'batch_size': 8, 'num_workers': 2}\n","num_epochs = 1\n","num_classes = 2\n","num_coord = 4\n","\n","# instantiate data loaders\n","# split the dataset in train and test set\n","indices = torch.randperm(len(ds)).tolist()\n","dataset = torch.utils.data.Subset(ds, indices[:-50])\n","dataset_test = torch.utils.data.Subset(ds_test, indices[-50:])\n","\n","# define training and validation data loaders\n","data_loader_training = torch.utils.data.DataLoader(dataset, shuffle=True, collate_fn=collate_fn, **params)\n","data_loader_test = torch.utils.data.DataLoader(dataset_test, shuffle=True, collate_fn=collate_fn, **params)"],"metadata":{"id":"cm3-m2n_RgfT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"H4SrIPBsTRQu"},"execution_count":null,"outputs":[]}]}