{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FasterRCNN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1i9J2iFNbyJj-Z2c4DjQE6Nh6jchrSGsA","authorship_tag":"ABX9TyNdDUcqQXeXjV4niqhd/xxD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdRwPIDA0nWB","executionInfo":{"status":"ok","timestamp":1645724510431,"user_tz":-60,"elapsed":18698,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"ed084c7a-b121-4bda-b28b-8d3674b0d0a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","import xml.etree.ElementTree as ET\n","import pickle as cPickle\n","import argparse\n","\n","import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.rpn import AnchorGenerator\n","import torchvision.transforms as T\n","\n","import torch.nn as nn\n","import torch\n","\n","!pip install albumentations==0.4.6\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import sys\n","sys.path.insert(0,\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/dataset\")"],"metadata":{"id":"4KJo128n1INW","executionInfo":{"status":"ok","timestamp":1646130133149,"user_tz":-60,"elapsed":3494,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a92929b8-ee58-4099-bc99-9079a06f222a"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n","Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.1.post1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.2.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n"]}]},{"cell_type":"code","source":["# Manage multiple versions of python with pip\n","# py -3.8 -m pip install package\n","#https://stackoverflow.com/questions/2812520/dealing-with-multiple-python-versions-and-pip\n","# Inspired by torchvision example: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n","\n","class BirdDataset(torch.utils.data.Dataset):\n","    \"\"\"Class to charecterize the bird dataset\"\"\"\n","\n","    def __init__(self, root_dir, transforms=None):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.root = root_dir\n","        self.transforms = transforms\n","        \n","        self.imgs = list(sorted(os.listdir(os.path.join(root_dir, \"all_images\")), key=lambda x: int(os.path.splitext(x)[0])))  # list of all image names - jpg\n","        self.boxes = list(sorted(os.listdir(os.path.join(root_dir, \"all_labels\")), key=lambda x: int(os.path.splitext(x)[0]))) # list of all image names - xml\n","    \n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Loads and returns a sample from the dataset at the given index idx\"\"\"\n","        # load images and boxes\n","        img_path = os.path.join(self.root, \"all_images\", self.imgs[idx])\n","        box_path = os.path.join(self.root, \"all_labels\", self.boxes[idx])\n","        # print(\"Image path\", img_path)\n","        # print(type(cv2.imread(img_path, cv2.IMREAD_COLOR)))\n","        # img = Image.open(img_path).convert(\"RGB\")\n","        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        img /= 255.0\n","        \n","        # get boxes for each bird\n","        document = ET.parse(box_path)\n","        root = document.getroot()\n","        boxes = []\n","        for item in root.findall(\".//object/bndbox\"):\n","            xmin = float(item.find('xmin').text)\n","            xmax = float(item.find('xmax').text)\n","            ymin = float(item.find('ymin').text)\n","            ymax = float(item.find('ymax').text)\n","\n","            box = [xmin, ymin, xmax, ymax]\n","            boxes.append(box)\n","        num_objs = len(boxes)\n","\n","        # convert everything into a torch.Tensor\n","        image_id = torch.tensor([idx+1])\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.ones((num_objs,), dtype=torch.int64) # only one class : a bird\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","\n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        # target[\"masks\"] = masks\n","        target[\"image_id\"] = image_id\n","        target[\"area\"] = area\n","        # target[\"iscrowd\"] = iscrowd\n","\n","        if self.transforms is not None:\n","            # img = self.transforms(img)\n","            sample = {\n","                'image': img,\n","                'bboxes': target['boxes'],\n","                'labels': labels\n","            }\n","            sample = self.transforms(**sample)\n","\n","            img = sample['image']\n","            if len(sample['bboxes']) == 0: # \n","                target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n","            else:\n","                target['boxes'] = torch.tensor(sample['bboxes'])\n","\n","        return img, target"],"metadata":{"id":"SIZlc4Ut1rTV","executionInfo":{"status":"ok","timestamp":1646130133388,"user_tz":-60,"elapsed":4,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["def get_model(trained=True, save_path=None):\n","  device = 'cpu'\n","  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","  num_classes = 2  # 1 class (bird) + background\n","\n","  # get number of input features for the classifier\n","  in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","  # replace the pre-trained head with a new one\n","  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","  model.to(device)\n","\n","  if trained:\n","    if save_path == None: print(\"No path tto the saved model\")\n","    model.load_state_dict(torch.load(save_path))\n","\n","  return model"],"metadata":{"id":"do6oPKRF4jfj","executionInfo":{"status":"ok","timestamp":1646130135130,"user_tz":-60,"elapsed":314,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["#########\n","# UTILS #\n","#########\n","def collate_fn(batch):\n","  \"\"\"Necessary for DataLoader\"\"\"\n","  return tuple(zip(*batch))\n","\n","def get_transform(train):\n","  \"\"\"Tansform the training and test set. Data Augmentation is made here.\"\"\"\n","  transforms = []\n","  transforms.append(A.Resize(224, 224, interpolation = cv2.INTER_LANCZOS4))\n","  if train:\n","      print(\"\")\n","      # transforms.append(A.RandomCrop(width=576, height=576))\n","      # transforms.append(A.Flip(0.5))\n","      # transforms.append(A.Normalize(mean=[0.598, 0.554, 0.508], std=[0.090, 0.081, 0.076]))           \n","  transforms.append(ToTensorV2(p=1.0)) \n","  return A.Compose(transforms, bbox_params={'format': 'pascal_voc', 'min_visibility': 0.6, 'label_fields': ['labels']})\n","\n","def save_performance(score, filename):\n","  \"\"\"Save the scores (4) on a txt file of name filename\"\"\"\n","  return 0\n","\n","class Performance():\n","    \"\"\"Class to calculate and store the performance/score of a model\"\"\"\n","    def __init__(self, root_save=None, args=None):\n","        \"\"\"\n","        Args:\n","            root_save (string): \n","            params (callable, optional):\n","        \"\"\"\n","        self.root_save = root_save\n","        self.args = args\n","        self.train_score = [[],[],[],[]]\n","        self.validation_score = [[],[],[],[]]\n","        self.training_iou = []\n","        self.test_iou = []\n","        self.iterperepoch = 0\n","\n","    def add_score(self, values, training):\n","      \"\"\"\n","      Add the different score of the model for every iteration to the corresponding list of score\n","      values : dictionnary containing  'loss_classifier', 'loss_box_reg', 'loss_objectness', 'loss_rpn_box_reg'\n","      \"\"\"\n","      if training:\n","        self.train_score[0].append(values['loss_classifier'].cpu().detach().item())\n","        self.train_score[1].append(values['loss_box_reg'].cpu().detach().item())\n","        self.train_score[2].append(values['loss_objectness'].cpu().detach().item())\n","        self.train_score[3].append(values['loss_rpn_box_reg'].cpu().detach().item())\n","      else:    \n","        self.train_score[0].append(values['loss_classifier'].cpu().detach().item())\n","        self.train_score[1].append(values['loss_box_reg'].cpu().detach().item())\n","        self.train_score[2].append(values['loss_objectness'].cpu().detach().item())\n","        self.train_score[3].append(values['loss_rpn_box_reg'].cpu().detach().item())\n","    \n","    def add_accuracy(self, val, training):\n","      if training:\n","        self.training_iou.append(val)\n","      else:\n","        self.test_iou.append(val)\n","\n","    def save_class(self, obj):\n","        \"\"\"Save the class in txt file\"\"\"\n","        with open(self.root_save, 'wb') as outp:  # Overwrites any existing file.\n","            cPickle.dump(obj, outp, cPickle.HIGHEST_PROTOCOL)\n","\n","    def save_score(self, path):\n","      file = open(path, \"w+\")\n","      file.write(str(self.train_score))\n","      file.close()\n","\n","    def load(self):\n","        \"\"\"Load the class from txt file\"\"\"\n","        file = open(self.root_save,'rb')\n","        dataPickle = file.read()\n","        file.close()\n","        return cPickle.loads(dataPickle)\n","        # self.__dict__ = cPickle.loads(dataPickle)"],"metadata":{"id":"2ft0imD_SqZy","executionInfo":{"status":"ok","timestamp":1646130136245,"user_tz":-60,"elapsed":2,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# Global variable\n","ROOT_DIR_DATA = \"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/dataset\"\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/savedmodel\"\n","SAVEDMODEL_NAME = \"/fasterrcnn_SGD0005_Size224_Batch8_Epoch2.pth\"\n","SAVEDPERFORMANCE_NAME = \"/score_fasterrcnn_SGD0005_Size224_Batch8_Epoch2.pkl\"\n","\n","# instantiate dataset objects\n","ds = BirdDataset(ROOT_DIR_DATA, get_transform(train=True))\n","ds_test = BirdDataset(ROOT_DIR_DATA, get_transform(train=False))\n","\n","# set hyper-parameters\n","num_epochs = 1\n","num_classes = 2\n","num_coord = 4\n","num_workers = 2\n","batch_size = 8\n","\n","# instantiate data loaders\n","# split the dataset in train and test set\n","# random_seed = 1 # or any of your favorite number \n","# torch.manual_seed(random_seed)\n","indices = torch.randperm(len(ds)).tolist()\n","print(indices)\n","# torch.manual_seed(random_seed)\n","# indices = torch.randperm(len(ds)).tolist()\n","# print(indices)\n","# indices = torch.randperm(len(ds)).tolist()\n","# print(indices)\n","dataset = torch.utils.data.Subset(ds, indices[:-50])\n","dataset_test = torch.utils.data.Subset(ds_test, indices[-50:])\n","\n","# define training and validation data loaders\n","data_loader_training = torch.utils.data.DataLoader(dataset, shuffle=True, collate_fn=collate_fn, \n","                                                   num_workers=num_workers, batch_size=batch_size)\n","data_loader_test = torch.utils.data.DataLoader(dataset_test, shuffle=True, collate_fn=collate_fn,\n","                                               num_workers=num_workers, batch_size=batch_size)"],"metadata":{"id":"cm3-m2n_RgfT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646130139248,"user_tz":-60,"elapsed":221,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"b2c8057a-0ef3-4e8c-ef74-a40fe2f2d0df"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","[310, 147, 37, 16, 41, 362, 188, 563, 88, 101, 267, 2, 73, 510, 69, 457, 277, 479, 169, 48, 611, 49, 144, 167, 145, 311, 234, 356, 252, 640, 279, 339, 81, 95, 205, 493, 66, 396, 439, 507, 527, 505, 25, 114, 490, 246, 332, 301, 293, 496, 156, 495, 159, 170, 163, 39, 43, 209, 249, 639, 127, 90, 600, 40, 531, 477, 100, 0, 619, 171, 514, 417, 282, 75, 519, 436, 551, 501, 524, 626, 489, 31, 625, 152, 480, 429, 607, 258, 153, 383, 408, 582, 168, 122, 398, 482, 564, 71, 125, 465, 648, 424, 50, 485, 229, 299, 378, 56, 645, 53, 355, 351, 526, 216, 104, 597, 427, 124, 236, 376, 318, 141, 618, 516, 214, 302, 634, 403, 184, 307, 334, 116, 30, 406, 295, 309, 360, 452, 139, 401, 638, 180, 77, 186, 175, 358, 348, 59, 415, 410, 353, 96, 592, 192, 23, 430, 29, 537, 133, 473, 631, 433, 574, 61, 255, 87, 558, 312, 448, 207, 113, 630, 326, 363, 131, 17, 529, 317, 336, 342, 633, 460, 447, 431, 33, 397, 499, 603, 400, 469, 591, 164, 413, 5, 534, 15, 58, 146, 247, 599, 422, 313, 199, 4, 606, 554, 196, 290, 193, 129, 612, 305, 472, 233, 22, 327, 79, 423, 45, 300, 651, 440, 604, 508, 589, 20, 137, 287, 134, 62, 650, 547, 130, 330, 281, 283, 642, 338, 223, 44, 202, 314, 643, 435, 381, 416, 461, 108, 652, 85, 244, 208, 1, 644, 587, 462, 27, 67, 259, 34, 428, 24, 308, 331, 450, 598, 419, 365, 583, 148, 399, 140, 530, 18, 544, 373, 395, 528, 540, 467, 86, 627, 119, 389, 426, 83, 649, 550, 270, 278, 38, 248, 610, 206, 367, 132, 274, 213, 590, 595, 454, 568, 357, 380, 105, 636, 210, 303, 215, 297, 441, 118, 444, 319, 349, 552, 359, 391, 565, 543, 420, 343, 253, 232, 176, 478, 412, 99, 621, 628, 466, 486, 608, 46, 224, 160, 451, 103, 571, 542, 195, 377, 266, 239, 316, 21, 577, 352, 375, 402, 178, 366, 298, 55, 385, 68, 260, 567, 194, 622, 491, 521, 51, 150, 143, 237, 155, 8, 580, 407, 231, 523, 251, 121, 65, 520, 350, 340, 545, 82, 453, 13, 80, 179, 257, 97, 151, 256, 138, 241, 464, 63, 404, 586, 655, 112, 328, 370, 475, 522, 615, 226, 573, 553, 142, 481, 57, 623, 211, 12, 458, 594, 162, 321, 635, 470, 74, 173, 535, 393, 212, 185, 273, 93, 629, 593, 632, 654, 294, 418, 276, 557, 323, 584, 463, 548, 52, 191, 201, 483, 190, 98, 42, 442, 394, 647, 581, 220, 166, 115, 492, 7, 637, 341, 624, 84, 509, 200, 487, 272, 498, 218, 576, 165, 158, 502, 221, 289, 204, 315, 217, 28, 641, 47, 284, 541, 120, 324, 409, 616, 620, 304, 14, 60, 405, 504, 110, 333, 515, 556, 414, 271, 374, 382, 203, 291, 345, 262, 3, 437, 245, 484, 117, 390, 533, 538, 64, 617, 561, 238, 566, 322, 296, 497, 488, 372, 172, 347, 35, 228, 149, 182, 222, 518, 476, 181, 569, 36, 474, 335, 468, 570, 265, 261, 126, 346, 471, 227, 106, 536, 511, 128, 183, 368, 135, 269, 329, 506, 443, 240, 411, 364, 596, 562, 286, 32, 449, 9, 532, 575, 455, 288, 646, 242, 235, 292, 605, 275, 325, 555, 320, 264, 601, 219, 459, 494, 70, 456, 371, 177, 549, 306, 92, 546, 89, 154, 225, 268, 369, 386, 602, 445, 503, 344, 285, 434, 517, 379, 512, 72, 26, 432, 392, 123, 54, 539, 76, 6, 446, 136, 197, 19, 421, 560, 500, 614, 243, 254, 157, 588, 187, 513, 585, 198, 189, 578, 525, 438, 91, 11, 78, 337, 384, 107, 102, 161, 10, 111, 354, 230, 361, 653, 263, 559, 280, 609, 94, 613, 250, 174, 572, 387, 425, 388, 109, 579]\n"]}]},{"cell_type":"code","source":["#################\n","# Visualisation #\n","#################\n","device = \"cpu\"\n","images, targets = next(iter(data_loader_training))\n","# for images, targets in data_loader_training:\n","images = list(image.to(device) for image in images)\n","targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","# print(targets)\n","\n","for i in range(len(images)):\n","    # print(targets[i]['boxes'].size())\n","    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n","    image_id = targets[i]['image_id'].cpu().numpy().astype(np.int32)\n","    area = targets[i]['area'].cpu().numpy().astype(np.int32)\n","    print(image_id)\n","    # print(\"Area\", area)\n","    # print(images[i].size())\n","    sample = images[i].permute(1,2,0).cpu().numpy()\n","    # print(sample)\n","    # print(sample.shape)\n","    # print(\"Box\", boxes)\n","\n","    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","    sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n","    for box in boxes:\n","        cv2.rectangle(sample,\n","                    (int(box[0]), int(box[1])),\n","                    (int(box[2]), int(box[3])),\n","                    (1, 0, 0), 1)\n","        \n","    ax.imshow((sample * 255).astype(np.uint8))\n","    plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/\"+str(image_id)+\"area\" + \".png\")\n","    plt.show()"],"metadata":{"id":"CN4ImOk-22lV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#################################\n","# Training and Testing function #\n","#################################\n","def training(args):\n","  ds = BirdDataset(ROOT_DIR_DATA, get_transform(train=True))\n","  data_loader_training = torch.utils.data.DataLoader(dataset, shuffle=True, collate_fn=collate_fn, \n","                                                   num_workers=num_workers, batch_size=args.batch_size)\n","\n","  device = \"cpu\"\n","  model = get_model(trained=False)\n","\n","  params = [p for p in model.parameters() if p.requires_grad]\n","  optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","  # optimizer = torch.optim.Adam(params, lr=0.001)\n","  # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","  # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n","  lr_scheduler = None\n","\n","  itr = 1\n","  epoch_loss = 0\n","  perf = Performance(ROOT_DIR_SAVING + SAVEDPERFORMANCE_NAME, args)\n","\n","  for epoch in range(args.epochs):\n","      epoch_loss = 0\n","      iteration = 0\n","      model.train()\n","\n","      # Training\n","      for images, targets in data_loader_training:\n","          images = list(image.to(device) for image in images)\n","          targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","          # print(images)\n","\n","          loss_dict = model(images, targets) # returns losses and detections\n","          perf.add_score(loss_dict, training=True)\n","          print(\"Output model/Loss :\", loss_dict)\n","\n","          losses = sum(loss for loss in loss_dict.values())\n","          loss_value = losses.item()\n","\n","          epoch_loss += loss_value\n","\n","          optimizer.zero_grad()\n","          losses.backward()\n","          optimizer.step()\n","\n","          if itr % 50 == 0:\n","              print(f\"Iteration #{itr} loss: {loss_value}\")\n","              perf.save_class(perf)\n","              torch.save(model.state_dict(), ROOT_DIR_SAVING + SAVEDMODEL_NAME)         \n","              print(\"SavedOnce\")\n","\n","          itr += 1\n","          iteration += 1\n","\n","\n","      # Validation\n","      for images, targets in data_loader_test:\n","          images = list(image.to(device) for image in images)\n","          targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","          # print(images)\n","          with torch.no_grad():\n","            loss_dict = model(images, targets) # returns losses and detections\n","            perf.add_score(loss_dict, training=False)\n","            print(\"Validation Loss :\", loss_dict)\n","     \n","      # Update the learning rate\n","      if lr_scheduler is not None:\n","          lr_scheduler.step()\n","\n","      print(f\"Epoch #{epoch} loss: {epoch_loss/iteration}\")\n","      torch.save(model.state_dict(), ROOT_DIR_SAVING + SAVEDMODEL_NAME)\n","      if perf.iterperepoch == 0:\n","        perf.iterperepoch = iter\n","      perf.save_class(perf)\n","      print(\"SaveAtEpoch\")\n","\n","  torch.save(model.state_dict(), ROOT_DIR_SAVING + SAVEDMODEL_NAME)\n","  perf.save(perf)\n","  print(\"Training is over.\")\n","  print(\"The model is saved.\")\n","\n","def test(args):\n","  return 0"],"metadata":{"id":"H4SrIPBsTRQu","executionInfo":{"status":"ok","timestamp":1646130142579,"user_tz":-60,"elapsed":546,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Settings\n","    parser = argparse.ArgumentParser(description='Point Cloud Part Segmentation')\n","    parser.add_argument('-f') #https://stackoverflow.com/questions/42249982/systemexit-2-error-when-calling-parse-args-within-ipython?noredirect=1&lq=1\n","    parser.add_argument('--eval', type=bool,  default=True, help='Evaluate the model')\n","    parser.add_argument('--model', type=str, default='fasterrcnn', metavar='N',\n","                        choices=['fasterrcnn'], help='Model to use')\n","    parser.add_argument('--batch_size', type=int, default=8, metavar='batch_size',\n","                        help='Size of batch)')\n","    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n","                        help='Number of episode to train ')\n","    parser.add_argument('--use_sgd', type=bool, default=True,\n","                        help='Use SGD')\n","    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n","                        help='learning rate (default: 0.001, 0.1 if using sgd)')\n","    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n","                        help='SGD momentum (default: 0.9)')\n","    parser.add_argument('--scheduler', type=str, default='cos', metavar='N',\n","                        choices=['cos', 'step'],\n","                        help='Scheduler to use, [cos, step]')\n","\n","    args = parser.parse_args()\n","\n","    if args.eval:\n","      training(args)\n","    else:\n","      test(args)\n","    "],"metadata":{"id":"eux2UMEzi8kd","colab":{"base_uri":"https://localhost:8080/","height":227},"executionInfo":{"status":"error","timestamp":1646130283537,"user_tz":-60,"elapsed":137665,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"96a0dc97-9b6f-4d40-cdeb-bfa417e348e2"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Output model/Loss : {'loss_classifier': tensor(0.6677, grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(0.8651, grad_fn=<DivBackward0>), 'loss_objectness': tensor(0.0265, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(0.0191, dtype=torch.float64, grad_fn=<DivBackward0>)}\n","Iteration #1 loss: 1.5784430425741123\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"code","source":["perf1 = Performance(root_save=ROOT_DIR_SAVING + SAVEDPERFORMANCE_NAME)\n","perf1 = perf1.load()\n","print(perf1.train_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJNbxWZO2h7A","executionInfo":{"status":"ok","timestamp":1646130287749,"user_tz":-60,"elapsed":221,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"}},"outputId":"f57fc32d-12b2-46f6-a5ac-693d7b31a114"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.6677238941192627], [0.8651007413864136], [0.02647457644343376], [0.019143811998550773]]\n"]}]}]}