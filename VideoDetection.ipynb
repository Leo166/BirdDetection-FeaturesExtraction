{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_JwUsqQORslA"},"outputs":[],"source":["import cv2\n","import imutils\n","import glob\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torch\n","import torchvision\n","from torchvision.io import read_image\n","import torchvision.transforms as T\n","from torchvision.models.detection.rpn import AnchorGenerator\n","!pip install albumentations==0.4.6\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","import json\n","\n","from google.colab.patches import cv2_imshow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cf2OulvPEoep"},"outputs":[],"source":["# Requirement for SORT algorithm\n","!pip install filterpy==1.4.5\n","!pip install scikit-image==0.17.2\n","!pip install lap==0.4.0"]},{"cell_type":"markdown","source":["# Faster R-CNN models"],"metadata":{"id":"lIKtcWUNRNJB"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["392c702de4ba46b190673e65f6c48da1","7b52215c3dd74d469c2e182ebccc0cb6","253c86c2630a433592a8b75aaa64c060","ce79d2c6dcd64685afaeef9d86f59e71","2dcb5e3b787149c88449965beda73990","09b249dfe3e6460b9c1400742b81202c","2cc73e34f37f4143853532f2877a7dcc","b30a7b3d5d1141a78d44fc19544b3b4a","efce63d0a291451b8d9798073f505328","930bd3abdf28479b8f5fbdfadc433a21","4bf652f3dda84c719a33d5ff46ff57d6"]},"executionInfo":{"elapsed":8264,"status":"ok","timestamp":1661758699731,"user":{"displayName":"Adrien Delhez","userId":"11782226645884779261"},"user_tz":-120},"id":"EMDnnZh656g5","outputId":"b6d6a858-3484-4033-f28d-6737c3985cca"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"392c702de4ba46b190673e65f6c48da1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/74.2M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def get_MobileNetmodel(trained=True, save_path=None):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n","  # print(model)\n","\n","  num_classes = 2  # 1 class (bird) + background\n","\n","  # get number of input features for the classifier\n","  in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","  # replace the pre-trained head with a new one\n","  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","  if trained:\n","    if save_path == None: print(\"No path to the saved model\")\n","    else: model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n","  model.to(device)\n","  return model\n","\n","model = get_MobileNetmodel(trained=False)\n","# print(model)\n","\n","def get_MobileNet320model(trained=True, save_path=None):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n","\n","  num_classes = 2  # 1 class (bird) + background\n","\n","  # get number of input features for the classifier\n","  in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","  # replace the pre-trained head with a new one\n","  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","  model.to(device)\n","  if trained:\n","    if save_path == None: print(\"No path to the saved model\")\n","    else: model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n","  \n","  return model\n","\n","def get_ResNet50model(trained=True, save_path=None):\n","  #device = 'cpu'\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  print(\"Device\", device)\n","  \n","  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","  #model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=True)\n","  num_classes = 2  # 1 class (bird) + background\n","\n","  # get number of input features for the classifier\n","  in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","  # replace the pre-trained head with a new one\n","  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","  if trained:\n","      if save_path == None: print(\"No path to the saved model\")\n","      else: model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n","\n","  model.to(device)\n","  return model\n","\n","def transforminput(maxsize):\n","    transforms = []\n","    # transforms.append(A.PadIfNeeded(maxsize, maxsize, border_mode=cv2.BORDER_CONSTANT))\n","    # transforms.append(A.Resize(576, 576))\n","    transforms.append(A.CenterCrop(width=576, height=576))\n","    return A.Compose(transforms)\n","\n","def transformback(size):\n","    transforms = []\n","    transforms.append(A.Resize(size[0], size[1]))        \n","    transforms.append(ToTensorV2(p=1.0)) \n","    return A.Compose(transforms, bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","\n","toTensor = T.Compose([T.ToTensor()])"]},{"cell_type":"markdown","source":["# SORT algotihm (test - not clean)"],"metadata":{"id":"vxvJCsudRGNP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vu4d7yhr5WrA"},"outputs":[],"source":["def video_detection(videopath):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  model = get_ResNet50model(trained=True, save_path=ROOT_DIR_SAVING + MODEL_NAME)\n","  model.eval()\n","  vid = cv2.VideoCapture(videopath)\n","  all_frames = []\n","  all_boxes = []\n","  all_scores = [] \n","  while vid.isOpened():\n","      ret, orig_frame = vid.read()\n","      if ret == True:\n","          orig_shape = orig_frame.shape\n","          frame = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2RGB).astype(np.float32)\n","          frame /= 255.0\n","          frame = cv2.resize(frame, (576,576))\n","          tensorframe = toTensor(frame)\n","          tensorframe = tensorframe[None, ...]\n","          outputs = model(tensorframe)\n","          outputs = [{k: v.to(device) for k, v in t.items()} for t in outputs]\n","\n","\n","          score  = outputs[0]['scores']\n","          predboxes = outputs[0]['boxes']\n","          labels = outputs[0][\"labels\"].cpu().detach().numpy().astype(np.int32)\n","          print(\"Score\", score)\n","          # print(\"True boxes\", boxes)\n","\n","          keepidx = torchvision.ops.nms(predboxes, score, 0.1).cpu().detach().numpy().astype(np.int32)\n","          score  = outputs[0]['scores'].cpu().detach().numpy()\n","          output = outputs[0]['boxes'].cpu().detach().numpy().astype(np.int32)\n","          score = score[keepidx]\n","          output = output[keepidx]\n","          labels = labels[keepidx]\n","\n","          confidenceidx = [idx for idx, elt in enumerate(score) if elt >= 0.5]\n","          output = output[confidenceidx]\n","          score = score[confidenceidx]\n","          labels = labels[confidenceidx]\n","          print(\"NMSScore\", score)\n","\n","\n","          area = (output[:, 3] - output[:, 1]) * (output[:, 2] - output[:, 0])\n","          zeroarea_filter = []\n","          for i in area:\n","              if i == 0:\n","                  zeroarea_filter.append(False)\n","              else:\n","                  zeroarea_filter.append(True)\n","\n","          sample = {\n","              'image': frame,\n","              'bboxes': output[zeroarea_filter],\n","              'labels': labels[zeroarea_filter]\n","          }\n","\n","          size = (orig_shape[0], orig_shape[1]) # height, width\n","          print(\"Size of original image\", size)\n","          sample = transformback(size)(**sample)\n","          boxes = sample['bboxes']\n","\n","          all_frames.append(orig_frame)\n","          all_boxes.append(boxes)\n","          all_scores.append(score)\n","      else:\n","          print(\"Error - Problem with reading the video\")\n","          break\n","  return all_frames, all_boxes, all_scores\n","\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/\"\n","MODEL_NAME = \"fasterrcnn_SGD0005_SchedulerNone_Size576_Batch8_Epoch50_8020.pth\"\n","videopath = \"/content/drive/MyDrive/Thesis/videos/A6.mp4\"\n","all_frames, all_boxes, all_scores = video_detection(videopath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRJIyDk1gOUv"},"outputs":[],"source":["def getSORTformat(all_boxes, all_scores):\n","  all_SORTboxes = []\n","  for idx_frame, i in enumerate(all_boxes): # i is list boxes in frame\n","    frame = []\n","    for idx_obj, obj in enumerate(i): # obj is box in list boxes\n","      obj = list(obj)\n","      frame.append(list(obj) + [all_scores[idx_frame][idx_obj]])\n","    all_SORTboxes.append(frame)\n","  return all_SORTboxes\n","\n","all_frames, all_boxes, all_scores = all_frames.copy(), all_boxes.copy(), all_scores.copy()\n","all_SORTboxes = getSORTformat(all_boxes, all_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1646749239944,"user":{"displayName":"Adrien Delhez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11782226645884779261"},"user_tz":-60},"id":"sQsIUDNzEQHm","outputId":"40d863b6-8767-4605-da41-62bf1d0327d0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/sort.py:74: RuntimeWarning: invalid value encountered in true_divide\n","  r = w / float(h)\n"]}],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/')\n","from sort import *\n","\n","#create instance of SORT\n","mot_tracker = Sort(max_age=10, min_hits=1) \n","\n","# get detections\n","#...\n","\n","# update SORT\n","all_updateboxes = []\n","initial_frame = True\n","for i in all_SORTboxes:\n","  if (len(i) == 0) & initial_frame:\n","    track_bbs_ids = mot_tracker.update(np.array(i))\n","  if (len(i) != 0) & initial_frame:\n","    initial_frame = False\n","  if (len(i) == 0) & (not initial_frame):\n","    i = np.array([[0,0,0,0,0]])\n","    track_bbs_ids = mot_tracker.update(np.array(i))\n","  else:\n","    track_bbs_ids = mot_tracker.update(np.array(i))\n","  all_updateboxes.append(track_bbs_ids)\n","# print(all_updateboxes[0:15])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4DEBTow6JE_"},"outputs":[],"source":["all_box_frames = []\n","for idx_frame, orig_frame in enumerate(all_frames):\n","  # print(idx_frame)\n","  for idx, box in enumerate(all_updateboxes[idx_frame]):\n","    print(box)\n","    cv2.rectangle(orig_frame,\n","                (int(box[0]), int(box[1])),\n","                (int(box[2]), int(box[3])),\n","                (255, 0, 0), 5)\n","  # print(\"ok\")\n","  imS = cv2.resize(orig_frame, (960, 540))\n","  all_box_frames.append(orig_frame)\n","  cv2_imshow(imS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-9IRjzUxXoC"},"outputs":[],"source":["SAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/prediction/predictiontrackingA5_Batch8_Epoch50_576.mp4\"\n","size = (4096, 2160)\n","out = cv2.VideoWriter(SAVE_PATH,cv2.VideoWriter_fourcc(*'mp4v'), 20, size)\n"," \n","for i in range(len(all_box_frames)):\n","    out.write(all_box_frames[i])\n","out.release()"]},{"cell_type":"markdown","source":["# Video Detection"],"metadata":{"id":"Zg6jbysgPxGG"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SaXlwNZLQIFP"},"outputs":[],"source":["###################\n","# Video Detection #\n","###################\n","# ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/\"\n","# MODEL_NAME = \"bestfasterrcnnv2_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch50_8020_rc.pth\"\n","# MODEL_NAME = \"bestMobileNetv3_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100_8020_rc.pth\"\n","# MODEL_NAME = \"bestMobileNetv3320_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100_8020_rc.pth\"\n","\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/v1/\"\n","MODEL_NAME = \"bestfasterrcnnv1_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","# MODEL_NAME = \"bestfasterrcnnv1_MobileNetv3_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","# model = get_ResNet50model(trained=True, save_path=ROOT_DIR_SAVING + MODEL_NAME)\n","model = get_MobileNetmodel(trained=True, save_path=ROOT_DIR_SAVING + MODEL_NAME)\n","model.eval()\n","\n","print(\"------------Load------------\")\n","\n","# vid = cv2.VideoCapture(\"../dataset/Personal_videos/videos/VID_20211105_120528.mp4\")\n","# vid = cv2.VideoCapture(\"/content/drive/MyDrive/Thesis/videos/test1.mp4\")\n","# vid = cv2.VideoCapture(\"/content/drive/MyDrive/Thesis/videos/test2.mp4\")\n","vid = cv2.VideoCapture(\"/content/drive/MyDrive/Thesis/videos/vid7.mp4\")\n","# cv2.namedWindow(\"img\", cv2.WINDOW_NORMAL)\n","iter = 0\n","all_frames = []\n","all_boxes = []\n","all_birdamt = []\n","imgcount = 1\n","while vid.isOpened():\n","    # print(\"Enter\")\n","    ret, orig_frame = vid.read()\n","    # print(ret)\n","    if ret == True:\n","        orig_shape = orig_frame.shape\n","        size = (orig_shape[0], orig_shape[1])\n","        frame = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        # frame = transforminput(max(orig_shape))(image=frame)['image']\n","        # frame = frame[0:0+1576, 0:0+1576]\n","        # cv2_imshow(frame)\n","        # cv2.imwrite(\"/content/drive/MyDrive/Thesis/videos/video2image/v5/v5img\"+str(imgcount)+\".jpg\", orig_frame)\n","        # imgcount += 1\n","\n","        frame /= 255.0\n","        # frame = cv2.resize(frame, (32*192,32*108))\n","        tensorframe = toTensor(frame)\n","        tensorframe = tensorframe[None, ...].cuda()\n","        outputs = model(tensorframe)\n","        outputs = [{k: v.to(device) for k, v in t.items()} for t in outputs]\n","\n","        score  = outputs[0]['scores']\n","        predboxes = outputs[0]['boxes']\n","        labels = outputs[0][\"labels\"].cpu().detach().numpy().astype(np.int32)\n","        print(\"Score\", score)\n","        # print(\"True boxes\", boxes)\n","\n","        keepidx = torchvision.ops.nms(predboxes, score, 0.2).cpu().detach().numpy().astype(np.int32)\n","        score  = outputs[0]['scores'].cpu().detach().numpy()\n","        output = outputs[0]['boxes'].cpu().detach().numpy().astype(np.int32)\n","        score = score[keepidx]\n","        output = output[keepidx]\n","        labels = labels[keepidx]\n","\n","        confidenceidx = [idx for idx, elt in enumerate(score) if elt >= 0.1]\n","        output = output[confidenceidx]\n","        score = score[confidenceidx]\n","        labels = labels[confidenceidx]\n","        print(\"NMSScore\", score)\n","\n","        sample = {\n","            'image': frame,\n","            'bboxes': output,\n","            'labels': labels\n","        }\n","        print(size)\n","        sample = transformback(size)(**sample)\n","        boxes = sample['bboxes']\n","\n","        # loop over the boxes\n","        for idx, box in enumerate(boxes):\n","            cv2.rectangle(orig_frame,\n","                        (int(box[0]), int(box[1])),\n","                        (int(box[2]), int(box[3])),\n","                        (255, 0, 0), 5)\n","            all_birdamt.append(len(boxes))\n","        #     cv2.putText(orig_frame, 'Bird '+ str(round(score[idx],3)), (int(box[0]), int(box[1])-15), cv2.FONT_HERSHEY_SIMPLEX, 1.6, (255,0,0), 3)\n","        # for box in output:\n","        #     cv2.rectangle(frame,\n","        #                 (int(box[0]), int(box[1])),\n","        #                 (int(box[2]), int(box[3])),\n","        #                 (0, 0, 255), 1)\n","        all_frames.append(orig_frame)\n","        all_boxes.append(boxes)\n","        # else: break\n","        # print(\"Frame\" + str(iter))\n","        iter += 1\n","        imS = cv2.resize(orig_frame, (960, 540))\n","        cv2_imshow(imS)\n","        cv2.waitKey(1)\n","        # # Press Q on keyboard to  exit\n","        # if cv2.waitKey(500) & 0xFF == ord('q'):\n","        #     break\n","        if 0xFF == ord('q'):\n","            break\n","    else:\n","        break\n","\n","print(\"Save the all the boxes and the counting into a file csv\")\n","PSAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/presentationvideos/cigonyes.csv\"\n","data = np.asarray([[all_birdamt, all_boxes]])\n","np.savetxt(PSAVE_PATH, data, delimiter=',')\n","\n","# Load the txt file\n","data = np.loadtxt(PSAVE_PATH, delimiter=',')\n","print(data)\n","\n","\n","print(\"----------Prediction Done----------\")\n","# SAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/prediction/predictionA10_Batch8_Epoch50_224.mp4\"\n","SAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/prediction/predvid7Mobile.mp4\"\n","size = (size[1], size[0])\n","out = cv2.VideoWriter(SAVE_PATH,cv2.VideoWriter_fourcc(*'mp4v'), 20, size)\n"," \n","for i in range(len(all_frames)):\n","    out.write(all_frames[i])\n","out.release()\n","print(\"--------------Saved----------------\")"]},{"cell_type":"code","source":["# Code to save boxes/bird amount : better than saving the processed video\n","PSAVE_PATH = \"/content/drive/MyDrive/Thesis/videos/presentationvideos/cigonyes.npy\"\n","all_birdamt = [50,80,50,90,60,50]\n","all_boxes = [[[2,5,1,6]], [[4,5,8,45], [4,9,0,70]]]\n","data = np.asarray([all_birdamt, all_boxes])\n","import pickle as cPickle\n","with open(PSAVE_PATH, 'wb') as outp:  # Overwrites any existing file.\n","    cPickle.dump(data, outp, cPickle.HIGHEST_PROTOCOL)\n","\n","# Load the txt file\n","file = open(PSAVE_PATH,'rb')\n","dataPickle = file.read()\n","file.close()\n","res = cPickle.loads(dataPickle)\n","print(res)\n","print(res[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-MpKqu8U0A7m","executionInfo":{"status":"ok","timestamp":1661764076561,"user_tz":-120,"elapsed":419,"user":{"displayName":"Adrien Delhez","userId":"11782226645884779261"}},"outputId":"e8571186-1df5-4694-cc32-d07d1e0c4e03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[list([50, 80, 50, 90, 60, 50])\n"," list([[[2, 5, 1, 6]], [[4, 5, 8, 45], [4, 9, 0, 70]]])]\n","[50, 80, 50, 90, 60, 50]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  after removing the cwd from sys.path.\n"]}]},{"cell_type":"markdown","source":["# Image Detection"],"metadata":{"id":"bf0DX56WPl9I"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5gQEuVvtsr1"},"outputs":[],"source":["###################\n","# Image Detection #\n","###################\n","import time\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/\"\n","# MODEL_NAME = \"bestfasterrcnnv2_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch50_8020_rc.pth\"\n","MODEL_NAME = \"bestMobileNetv3_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100_8020_rc.pth\"\n","# MODEL_NAME = \"bestMobileNetv3320_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100_8020_rc.pth\"\n","\n","# ROOT_DIR_SAVING = \"/content/drive/MyDrive/Thesis/savedmodel/v1/\"\n","# MODEL_NAME = \"bestfasterrcnnv1_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","# MODEL_NAME = \"bestfasterrcnnv1_MobileNetv3_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","# MODEL_NAME = \"bestfasterrcnnv1_MobileNetv3320_SGD00005_SchedulerReduceLR_Size576_Batch8_Epoch100.pth\"\n","# MODEL_NAME = \"fasterrcnn_SGD0005_SchedulerNone_Size224_Batch8_Epoch25.pth\"\n","# MODEL_NAME = \"bestfasterrcnnv2_SGD0005_SchedulerReduceLR_Size576_Batch8_Epoch50_8020_rc.pth\"\n","# model = get_ResNet50model(trained=True, save_path=ROOT_DIR_SAVING + MODEL_NAME)\n","model = get_MobileNetmodel(trained=True, save_path=ROOT_DIR_SAVING + MODEL_NAME)\n","model.eval()\n","\n","def imagedetection(orig_frame):\n","  orig_frame = orig_frame[120:120+576, 600:600+576] #Pigeon\n","  # orig_frame = orig_frame[1000:1000+576, 1800:1800+576] #v3\n","  # orig_frame = orig_frame[700:700+576, 1850:1850+576] #v1\n","  # orig_frame = orig_frame[100:100+576, 530:530+576] #imgAU1\n","  # cv2_imshow(orig_frame)\n","  orig_shape = orig_frame.shape\n","  print(orig_shape)\n","  frame = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2RGB).astype(np.float32)\n","  frame /= 255.0\n","  print(frame.shape)\n","  # frame = cv2.resize(frame, (576, 576))\n","  # cv2_imshow(frame*255)\n","  # cv2_imshow(frame*255)\n","  tensorframe = toTensor(frame)\n","  tensorframe = tensorframe[None, ...].to(device)\n","  \n","  start = time.time()\n","  outputs = model(tensorframe)\n","  end = time.time()\n","  print(\"Time for one image\", end - start)\n","  outputs = [{k: v.to(device) for k, v in t.items()} for t in outputs]\n","\n","  score  = outputs[0]['scores']\n","  predboxes = outputs[0]['boxes']\n","\n","  keepidx = torchvision.ops.nms(predboxes, score, 0.2).cpu().detach().numpy().astype(np.int32)\n","  score  = outputs[0]['scores'].cpu().detach().numpy()\n","  output = outputs[0]['boxes'].cpu().detach().numpy().astype(np.int32)\n","  score = score[keepidx]\n","  output = output[keepidx]\n","\n","  confidenceidx = [idx for idx, elt in enumerate(score) if elt >= 0.1]\n","  output = output[confidenceidx]\n","  score = score[confidenceidx]\n","\n","  fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","  sample = cv2.cvtColor(orig_frame, cv2.COLOR_BGR2RGB)\n","  print(\"Size\", sample.shape)\n","  for box in output:\n","      cv2.rectangle(sample,\n","                  (int(box[0]), int(box[1])),\n","                  (int(box[2]), int(box[3])),\n","                  (0, 0, 255), 2)\n","      # cv2.putText(sample, str(round(score[idx],3)), (int(box[0]), int(box[1])-6), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (1,0,0), 1)\n","      \n","  ax.imshow((sample).astype(np.uint8))\n","  # plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/\"+str(image_id)+\"area\" + \".png\")\n","  ax.axis('off')\n","  # plt.savefig(\"/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/imagerapport/imgpigeonv2.pdf\", bbox_inches='tight')\n","  plt.show()\n","\n","# orig_frame = cv2.imread('/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/dataset/all_images/96.jpg', 1)\n","# orig_frame = cv2.imread('/content/drive/MyDrive/Thesis/videos/video2image/v5/v5img12.jpg', 1)\n","# orig_frame = cv2.imread('/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/imgAU1.jpg', 1)\n","orig_frame = cv2.imread('/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/ngbird1.JPG', 1)\n","# orig_frame = cv2.imread('/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/flyingbirdimg1.jpg', 1)\n","# orig_frame = cv2.imread('/content/drive/MyDrive/Github/BirdDetection-FeaturesExtraction/images/COCO1.jpg', 1)\n","imagedetection(orig_frame)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"mount_file_id":"132SZ3Qq0lPhQjPv6cDSjHNwHVHnVSCkl","authorship_tag":"ABX9TyNyvsFT/Psm2ZNpTLma3Ye3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"09b249dfe3e6460b9c1400742b81202c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"253c86c2630a433592a8b75aaa64c060":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b30a7b3d5d1141a78d44fc19544b3b4a","max":77844807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_efce63d0a291451b8d9798073f505328","value":77844807}},"2cc73e34f37f4143853532f2877a7dcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dcb5e3b787149c88449965beda73990":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"392c702de4ba46b190673e65f6c48da1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b52215c3dd74d469c2e182ebccc0cb6","IPY_MODEL_253c86c2630a433592a8b75aaa64c060","IPY_MODEL_ce79d2c6dcd64685afaeef9d86f59e71"],"layout":"IPY_MODEL_2dcb5e3b787149c88449965beda73990"}},"4bf652f3dda84c719a33d5ff46ff57d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b52215c3dd74d469c2e182ebccc0cb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09b249dfe3e6460b9c1400742b81202c","placeholder":"​","style":"IPY_MODEL_2cc73e34f37f4143853532f2877a7dcc","value":"100%"}},"930bd3abdf28479b8f5fbdfadc433a21":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b30a7b3d5d1141a78d44fc19544b3b4a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce79d2c6dcd64685afaeef9d86f59e71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_930bd3abdf28479b8f5fbdfadc433a21","placeholder":"​","style":"IPY_MODEL_4bf652f3dda84c719a33d5ff46ff57d6","value":" 74.2M/74.2M [00:02&lt;00:00, 26.9MB/s]"}},"efce63d0a291451b8d9798073f505328":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}